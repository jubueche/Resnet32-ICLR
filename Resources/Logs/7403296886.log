Sender: LSF System <lsfadmin@zhcc008>
Subject: Job 777163: <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.056 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=7403296886> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.056 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=7403296886> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:13 2021
Job was executed on host(s) <zhcc008>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:14 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:14 2021
Terminated at Tue Nov 23 12:50:29 2021
Results reported at Tue Nov 23 12:50:29 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.056 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=7403296886
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   225.98 sec.
    Max Memory :                                 2025 MB
    Average Memory :                             1348.08 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                133
    Run time :                                   75 sec.
    Turnaround time :                            76 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 94.280
 * Prec@1 15.320
 * Prec@1 16.460
 * Prec@1 14.220
 * Prec@1 16.900
 * Noisy Prec@1 15.86
After loading, with clipping@2.00 15.32 w/o 94.28 noisy: 15.86pm1.44
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.607 (0.607)	Data 0.503 (0.503)	Loss 0.8340 (0.8340)	Prec@1 87.109 (87.109)
Epoch: [60][17/176]	Time 0.078 (0.112)	Data 0.002 (0.030)	Loss 0.6348 (0.7714)	Prec@1 91.016 (91.233)
Epoch: [60][34/176]	Time 0.071 (0.095)	Data 0.001 (0.016)	Loss 0.5649 (0.7177)	Prec@1 90.234 (90.692)
Epoch: [60][51/176]	Time 0.083 (0.088)	Data 0.002 (0.011)	Loss 0.4658 (0.6567)	Prec@1 94.141 (91.556)
Epoch: [60][68/176]	Time 0.083 (0.086)	Data 0.002 (0.009)	Loss 0.4399 (0.6114)	Prec@1 96.094 (92.238)
Epoch: [60][85/176]	Time 0.088 (0.085)	Data 0.002 (0.007)	Loss 0.5284 (0.5883)	Prec@1 95.312 (92.882)
Epoch: [60][102/176]	Time 0.085 (0.084)	Data 0.002 (0.006)	Loss 0.4683 (0.5781)	Prec@1 96.484 (93.010)
Epoch: [60][119/176]	Time 0.071 (0.083)	Data 0.001 (0.006)	Loss 0.4439 (0.5662)	Prec@1 93.750 (93.086)
Epoch: [60][136/176]	Time 0.071 (0.081)	Data 0.001 (0.005)	Loss 0.5157 (0.5533)	Prec@1 93.750 (93.237)
Epoch: [60][153/176]	Time 0.071 (0.080)	Data 0.001 (0.005)	Loss 0.3737 (0.5460)	Prec@1 95.703 (93.349)
Epoch: [60][170/176]	Time 0.070 (0.080)	Data 0.000 (0.005)	Loss 0.4115 (0.5365)	Prec@1 96.484 (93.492)
 * Prec@1 84.600
 * Prec@1 83.540
 * Prec@1 82.760
 * Prec@1 83.680
 * Noisy Prec@1 83.33
	 * New best: 83.32667
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.565 (0.565)	Data 0.467 (0.467)	Loss 0.4696 (0.4696)	Prec@1 94.141 (94.141)
Epoch: [61][17/176]	Time 0.073 (0.106)	Data 0.001 (0.027)	Loss 0.4118 (0.4301)	Prec@1 97.266 (95.508)
Epoch: [61][34/176]	Time 0.082 (0.093)	Data 0.001 (0.015)	Loss 0.4134 (0.4208)	Prec@1 95.312 (95.446)
Epoch: [61][51/176]	Time 0.071 (0.089)	Data 0.001 (0.010)	Loss 0.4403 (0.4259)	Prec@1 95.703 (95.493)
Epoch: [61][68/176]	Time 0.079 (0.087)	Data 0.002 (0.008)	Loss 0.3872 (0.4262)	Prec@1 96.484 (95.324)
Epoch: [61][85/176]	Time 0.071 (0.086)	Data 0.001 (0.007)	Loss 0.4011 (0.4263)	Prec@1 95.703 (95.317)
Epoch: [61][102/176]	Time 0.072 (0.084)	Data 0.001 (0.006)	Loss 0.4125 (0.4298)	Prec@1 93.750 (95.324)
Epoch: [61][119/176]	Time 0.071 (0.082)	Data 0.001 (0.005)	Loss 0.4065 (0.4282)	Prec@1 95.703 (95.257)
Epoch: [61][136/176]	Time 0.071 (0.081)	Data 0.001 (0.005)	Loss 0.4633 (0.4277)	Prec@1 95.312 (95.230)
Epoch: [61][153/176]	Time 0.071 (0.080)	Data 0.001 (0.004)	Loss 0.4450 (0.4300)	Prec@1 93.359 (95.153)
Epoch: [61][170/176]	Time 0.070 (0.079)	Data 0.000 (0.004)	Loss 0.3629 (0.4290)	Prec@1 94.922 (95.098)
 * Prec@1 86.540
 * Prec@1 86.760
 * Prec@1 85.200
 * Prec@1 86.660
 * Noisy Prec@1 86.21
	 * New best: 86.20667
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.526 (0.526)	Data 0.454 (0.454)	Loss 0.3821 (0.3821)	Prec@1 97.656 (97.656)
Epoch: [62][17/176]	Time 0.076 (0.101)	Data 0.003 (0.027)	Loss 0.3594 (0.4042)	Prec@1 96.484 (95.204)
Epoch: [62][34/176]	Time 0.072 (0.087)	Data 0.002 (0.014)	Loss 0.3132 (0.3991)	Prec@1 97.656 (95.190)
Epoch: [62][51/176]	Time 0.072 (0.082)	Data 0.001 (0.010)	Loss 0.3966 (0.4006)	Prec@1 96.875 (95.275)
Epoch: [62][68/176]	Time 0.072 (0.081)	Data 0.002 (0.008)	Loss 0.3816 (0.3985)	Prec@1 94.922 (95.346)
Epoch: [62][85/176]	Time 0.074 (0.080)	Data 0.002 (0.007)	Loss 0.4597 (0.4000)	Prec@1 94.531 (95.358)
Epoch: [62][102/176]	Time 0.071 (0.079)	Data 0.001 (0.006)	Loss 0.3571 (0.4036)	Prec@1 96.484 (95.294)
Epoch: [62][119/176]	Time 0.072 (0.078)	Data 0.001 (0.005)	Loss 0.4090 (0.4033)	Prec@1 95.312 (95.251)
Epoch: [62][136/176]	Time 0.071 (0.077)	Data 0.001 (0.005)	Loss 0.4525 (0.4055)	Prec@1 94.141 (95.201)
Epoch: [62][153/176]	Time 0.071 (0.077)	Data 0.001 (0.004)	Loss 0.4216 (0.4068)	Prec@1 95.703 (95.143)
Traceback (most recent call last):
  File "trainer_resnet.py", line 222, in train
    output = model(input_var) #! This is inefficient, right?
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 151, in forward
    for t in chain(self.module.parameters(), self.module.buffers()):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1137, in buffers
    for name, buf in self.named_buffers(recurse=recurse):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1163, in named_buffers
    for elem in gen:
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1062, in _named_members
    if v is None or v in memo:
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/tensor.py", line 598, in __hash__
    from torch.overrides import has_torch_function, handle_torch_function
  File "<frozen importlib._bootstrap>", line 1019, in _handle_fromlist
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 138, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 222, in train
    output = model(input_var) #! This is inefficient, right?
KeyboardInterrupt
