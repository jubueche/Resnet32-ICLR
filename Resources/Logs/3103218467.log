Sender: LSF System <lsfadmin@zhcc012>
Subject: Job 768147: <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=3 -beta_robustness=0.25 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.05 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=3103218467> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=3 -beta_robustness=0.25 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.05 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=3103218467> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov  2 14:09:21 2021
Job was executed on host(s) <zhcc012>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov  2 14:09:22 2021
</u/jbu> was used as the home directory.
</u/jbu/Master-Thesis> was used as the working directory.
Started at Tue Nov  2 14:09:22 2021
Terminated at Tue Nov  2 14:13:13 2021
Results reported at Tue Nov  2 14:13:13 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=3 -beta_robustness=0.25 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.05 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=3103218467
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   359.18 sec.
    Max Memory :                                 2118 MB
    Average Memory :                             2002.31 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                131
    Run time :                                   233 sec.
    Turnaround time :                            232 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Master-Thesis/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 93.900
 * Prec@1 19.920
 * Prec@1 20.820
 * Prec@1 18.920
 * Prec@1 20.440
 * Noisy Prec@1 20.06
After loading, with clipping@2.00 19.92 w/o 93.90 noisy: 20.06pm1.01
current lr 2.00000e-02
Epoch: [60][0/176]	Time 1.096 (1.096)	Data 0.599 (0.599)	Loss 1.2315 (1.2315)	Prec@1 80.859 (80.859)
Epoch: [60][17/176]	Time 0.463 (0.502)	Data 0.001 (0.034)	Loss 0.8558 (1.1066)	Prec@1 85.547 (87.326)
Epoch: [60][34/176]	Time 0.464 (0.483)	Data 0.001 (0.018)	Loss 0.7612 (0.9831)	Prec@1 87.500 (87.422)
Epoch: [60][51/176]	Time 0.462 (0.477)	Data 0.001 (0.013)	Loss 0.7062 (0.8958)	Prec@1 89.062 (88.078)
Epoch: [60][68/176]	Time 0.461 (0.474)	Data 0.001 (0.010)	Loss 0.5756 (0.8341)	Prec@1 91.797 (88.836)
Epoch: [60][85/176]	Time 0.460 (0.471)	Data 0.001 (0.008)	Loss 0.7133 (0.7982)	Prec@1 89.844 (89.458)
Epoch: [60][102/176]	Time 0.459 (0.469)	Data 0.001 (0.007)	Loss 0.6149 (0.7742)	Prec@1 92.578 (89.749)
Epoch: [60][119/176]	Time 0.460 (0.468)	Data 0.001 (0.006)	Loss 0.6270 (0.7508)	Prec@1 89.453 (90.055)
Epoch: [60][136/176]	Time 0.461 (0.468)	Data 0.001 (0.006)	Loss 0.6369 (0.7323)	Prec@1 90.625 (90.311)
Epoch: [60][153/176]	Time 0.461 (0.467)	Data 0.001 (0.005)	Loss 0.5696 (0.7196)	Prec@1 91.016 (90.475)
Epoch: [60][170/176]	Time 0.457 (0.467)	Data 0.000 (0.005)	Loss 0.5638 (0.7073)	Prec@1 91.797 (90.662)
 * Prec@1 88.680
 * Prec@1 88.160
 * Prec@1 88.260
 * Prec@1 88.400
 * Noisy Prec@1 88.27
	 * New best: 88.27334
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.952 (0.952)	Data 0.475 (0.475)	Loss 0.6084 (0.6084)	Prec@1 85.156 (85.156)
Epoch: [61][17/176]	Time 0.460 (0.490)	Data 0.001 (0.027)	Loss 0.6574 (0.6382)	Prec@1 89.844 (91.211)
Epoch: [61][34/176]	Time 0.460 (0.476)	Data 0.001 (0.015)	Loss 0.5334 (0.6107)	Prec@1 94.141 (91.708)
Epoch: [61][51/176]	Time 0.460 (0.471)	Data 0.001 (0.010)	Loss 0.5433 (0.6001)	Prec@1 92.969 (91.925)
Epoch: [61][68/176]	Time 0.465 (0.469)	Data 0.001 (0.008)	Loss 0.5527 (0.5968)	Prec@1 92.969 (91.944)
Epoch: [61][85/176]	Time 0.462 (0.468)	Data 0.001 (0.007)	Loss 0.5283 (0.5879)	Prec@1 93.359 (92.192)
Epoch: [61][102/176]	Time 0.461 (0.467)	Data 0.001 (0.006)	Loss 0.5677 (0.5837)	Prec@1 90.625 (92.214)
Epoch: [61][119/176]	Time 0.460 (0.466)	Data 0.001 (0.005)	Loss 0.5198 (0.5783)	Prec@1 94.922 (92.350)
Epoch: [61][136/176]	Time 0.463 (0.465)	Data 0.001 (0.005)	Loss 0.5448 (0.5743)	Prec@1 92.578 (92.441)
Epoch: [61][153/176]	Time 0.460 (0.465)	Data 0.001 (0.004)	Loss 0.5903 (0.5746)	Prec@1 91.797 (92.388)
Epoch: [61][170/176]	Time 0.458 (0.465)	Data 0.000 (0.004)	Loss 0.4692 (0.5715)	Prec@1 93.359 (92.443)
 * Prec@1 89.160
 * Prec@1 89.060
 * Prec@1 89.160
 * Prec@1 89.180
 * Noisy Prec@1 89.13
	 * New best: 89.13333
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.966 (0.966)	Data 0.473 (0.473)	Loss 0.5063 (0.5063)	Prec@1 94.531 (94.531)
Epoch: [62][17/176]	Time 0.462 (0.490)	Data 0.001 (0.027)	Loss 0.4719 (0.5544)	Prec@1 96.094 (92.318)
Epoch: [62][34/176]	Time 0.461 (0.476)	Data 0.001 (0.015)	Loss 0.4890 (0.5536)	Prec@1 94.141 (92.623)
Epoch: [62][51/176]	Time 0.461 (0.471)	Data 0.001 (0.010)	Loss 0.6391 (0.5565)	Prec@1 90.625 (92.706)
Epoch: [62][68/176]	Time 0.461 (0.468)	Data 0.001 (0.008)	Loss 0.4761 (0.5561)	Prec@1 94.531 (92.572)
Epoch: [62][85/176]	Time 0.460 (0.467)	Data 0.001 (0.007)	Loss 0.5942 (0.5522)	Prec@1 91.797 (92.610)
Traceback (most recent call last):
  File "trainer_resnet.py", line 339, in <module>
    main()
  File "trainer_resnet.py", line 144, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 219, in train
    epoch=epoch - args.start_epoch
  File "/ibm/gpfs-homes/jbu/Master-Thesis/Losses/torch_loss.py", line 138, in compute_gradient_and_backward
    X
  File "/ibm/gpfs-homes/jbu/Master-Thesis/Losses/torch_loss.py", line 95, in _adversarial_loss
    output_theta_star = self.model_theta_star(X)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 159, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/Master-Thesis/Architectures/cifar_resnet.py", line 109, in forward
    out = self.layer1(out)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/Master-Thesis/Architectures/cifar_resnet.py", line 78, in forward
    out = self.bn2(self.conv2(out))
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 136, in forward
    self.weight, self.bias, bn_training, exponential_average_factor, self.eps)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/functional.py", line 2058, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
KeyboardInterrupt
