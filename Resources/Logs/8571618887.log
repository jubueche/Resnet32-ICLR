Sender: LSF System <lsfadmin@zhcc009>
Subject: Job 777167: <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=8571618887> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=8571618887> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:14 2021
Job was executed on host(s) <zhcc009>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:15 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:15 2021
Terminated at Tue Nov 23 12:50:29 2021
Results reported at Tue Nov 23 12:50:29 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=8571618887
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   242.63 sec.
    Max Memory :                                 2021 MB
    Average Memory :                             1359.77 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                131
    Run time :                                   79 sec.
    Turnaround time :                            75 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 94.280
 * Prec@1 15.320
 * Prec@1 17.480
 * Prec@1 13.220
 * Prec@1 19.060
 * Noisy Prec@1 16.59
After loading, with clipping@2.00 15.32 w/o 94.28 noisy: 16.59pm3.02
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.726 (0.726)	Data 0.550 (0.550)	Loss 0.8340 (0.8340)	Prec@1 87.109 (87.109)
Epoch: [60][17/176]	Time 0.083 (0.120)	Data 0.002 (0.032)	Loss 0.6192 (0.7704)	Prec@1 90.625 (91.037)
Epoch: [60][34/176]	Time 0.071 (0.101)	Data 0.001 (0.017)	Loss 0.6063 (0.7194)	Prec@1 89.844 (90.368)
Epoch: [60][51/176]	Time 0.072 (0.092)	Data 0.001 (0.012)	Loss 0.5171 (0.6586)	Prec@1 95.312 (91.181)
Epoch: [60][68/176]	Time 0.071 (0.087)	Data 0.001 (0.009)	Loss 0.4799 (0.6141)	Prec@1 96.875 (91.955)
Epoch: [60][85/176]	Time 0.071 (0.084)	Data 0.001 (0.008)	Loss 0.5695 (0.5914)	Prec@1 93.750 (92.496)
Epoch: [60][102/176]	Time 0.071 (0.082)	Data 0.001 (0.007)	Loss 0.5015 (0.5778)	Prec@1 92.188 (92.677)
Epoch: [60][119/176]	Time 0.071 (0.080)	Data 0.001 (0.006)	Loss 0.5059 (0.5654)	Prec@1 94.531 (92.852)
Epoch: [60][136/176]	Time 0.071 (0.079)	Data 0.001 (0.005)	Loss 0.5592 (0.5542)	Prec@1 90.625 (93.037)
Epoch: [60][153/176]	Time 0.071 (0.078)	Data 0.001 (0.005)	Loss 0.4285 (0.5457)	Prec@1 92.969 (93.195)
Epoch: [60][170/176]	Time 0.070 (0.078)	Data 0.000 (0.005)	Loss 0.4046 (0.5361)	Prec@1 97.266 (93.385)
 * Prec@1 85.080
 * Prec@1 82.180
 * Prec@1 81.540
 * Prec@1 82.020
 * Noisy Prec@1 81.91
	 * New best: 81.91333
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.579 (0.579)	Data 0.508 (0.508)	Loss 0.5124 (0.5124)	Prec@1 93.750 (93.750)
Epoch: [61][17/176]	Time 0.071 (0.100)	Data 0.001 (0.029)	Loss 0.4446 (0.4295)	Prec@1 96.875 (95.334)
Epoch: [61][34/176]	Time 0.071 (0.086)	Data 0.001 (0.016)	Loss 0.3907 (0.4365)	Prec@1 95.703 (94.855)
Epoch: [61][51/176]	Time 0.071 (0.081)	Data 0.001 (0.011)	Loss 0.4760 (0.4331)	Prec@1 95.312 (94.959)
Epoch: [61][68/176]	Time 0.071 (0.080)	Data 0.001 (0.009)	Loss 0.4414 (0.4335)	Prec@1 95.312 (94.956)
Epoch: [61][85/176]	Time 0.071 (0.078)	Data 0.001 (0.007)	Loss 0.4035 (0.4302)	Prec@1 97.266 (95.081)
Epoch: [61][102/176]	Time 0.076 (0.077)	Data 0.002 (0.006)	Loss 0.4230 (0.4305)	Prec@1 94.922 (95.036)
Epoch: [61][119/176]	Time 0.071 (0.076)	Data 0.001 (0.006)	Loss 0.4583 (0.4270)	Prec@1 94.531 (95.078)
Epoch: [61][136/176]	Time 0.071 (0.076)	Data 0.001 (0.005)	Loss 0.4639 (0.4251)	Prec@1 92.969 (95.113)
Epoch: [61][153/176]	Time 0.071 (0.076)	Data 0.001 (0.005)	Loss 0.4032 (0.4272)	Prec@1 96.094 (95.023)
Epoch: [61][170/176]	Time 0.070 (0.075)	Data 0.000 (0.004)	Loss 0.4249 (0.4265)	Prec@1 94.141 (95.000)
 * Prec@1 86.000
 * Prec@1 83.480
 * Prec@1 80.000
 * Prec@1 84.120
 * Noisy Prec@1 82.53
	 * New best: 82.53333
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.661 (0.661)	Data 0.581 (0.581)	Loss 0.3376 (0.3376)	Prec@1 97.656 (97.656)
Epoch: [62][17/176]	Time 0.083 (0.109)	Data 0.002 (0.034)	Loss 0.3347 (0.3922)	Prec@1 96.484 (95.985)
Epoch: [62][34/176]	Time 0.073 (0.092)	Data 0.002 (0.018)	Loss 0.3678 (0.3850)	Prec@1 97.266 (96.004)
Epoch: [62][51/176]	Time 0.072 (0.085)	Data 0.001 (0.013)	Loss 0.4650 (0.3921)	Prec@1 94.531 (95.756)
Epoch: [62][68/176]	Time 0.074 (0.082)	Data 0.001 (0.010)	Loss 0.3356 (0.3930)	Prec@1 94.531 (95.805)
Epoch: [62][85/176]	Time 0.074 (0.080)	Data 0.002 (0.008)	Loss 0.4408 (0.3942)	Prec@1 94.922 (95.721)
Epoch: [62][102/176]	Time 0.073 (0.079)	Data 0.001 (0.007)	Loss 0.4083 (0.3995)	Prec@1 97.656 (95.567)
Epoch: [62][119/176]	Time 0.071 (0.078)	Data 0.001 (0.006)	Loss 0.3800 (0.3973)	Prec@1 94.922 (95.531)
Epoch: [62][136/176]	Time 0.072 (0.077)	Data 0.001 (0.006)	Loss 0.3779 (0.3987)	Prec@1 95.312 (95.407)
Epoch: [62][153/176]	Time 0.074 (0.076)	Data 0.001 (0.005)	Loss 0.4813 (0.3990)	Prec@1 92.969 (95.353)
Epoch: [62][170/176]	Time 0.070 (0.076)	Data 0.000 (0.005)	Loss 0.3598 (0.3994)	Prec@1 96.094 (95.383)
 * Prec@1 87.220
 * Prec@1 85.200
 * Prec@1 84.400
 * Prec@1 84.160
 * Noisy Prec@1 84.59
	 * New best: 84.58667
current lr 2.00000e-02
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1002857e34d0>
Traceback (most recent call last):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1202, in __del__
    def __del__(self):
KeyboardInterrupt
Epoch: [63][0/176]	Time 0.643 (0.643)	Data 0.567 (0.567)	Loss 0.4705 (0.4705)	Prec@1 95.703 (95.703)
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 138, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 213, in train
    epoch=epoch - args.start_epoch
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Losses/torch_loss.py", line 153, in compute_gradient_and_backward
    nat_loss = self.natural_loss(model(X), y)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 151, in forward
    for t in chain(self.module.parameters(), self.module.buffers()):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1137, in buffers
    for name, buf in self.named_buffers(recurse=recurse):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1163, in named_buffers
    for elem in gen:
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1060, in _named_members
    members = get_members_fn(module)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1161, in <lambda>
    lambda module: module._buffers.items(),
KeyboardInterrupt
