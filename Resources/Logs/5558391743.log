Sender: LSF System <lsfadmin@zhcc005>
Subject: Job 777177: <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5558391743> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5558391743> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:17 2021
Job was executed on host(s) <zhcc005>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:18 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:18 2021
Terminated at Tue Nov 23 12:50:29 2021
Results reported at Tue Nov 23 12:50:29 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5558391743
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   218.49 sec.
    Max Memory :                                 2006 MB
    Average Memory :                             1384.58 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                133
    Run time :                                   70 sec.
    Turnaround time :                            72 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 94.280
 * Prec@1 15.320
 * Prec@1 16.920
 * Prec@1 14.680
 * Prec@1 16.360
 * Noisy Prec@1 15.99
After loading, with clipping@2.00 15.32 w/o 94.28 noisy: 15.99pm1.17
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.657 (0.657)	Data 0.551 (0.551)	Loss 0.8340 (0.8340)	Prec@1 87.109 (87.109)
Epoch: [60][17/176]	Time 0.071 (0.104)	Data 0.001 (0.032)	Loss 0.6453 (0.7624)	Prec@1 90.234 (90.907)
Epoch: [60][34/176]	Time 0.073 (0.088)	Data 0.002 (0.017)	Loss 0.5834 (0.7090)	Prec@1 88.281 (90.625)
Epoch: [60][51/176]	Time 0.071 (0.083)	Data 0.001 (0.012)	Loss 0.5250 (0.6558)	Prec@1 93.750 (91.188)
Epoch: [60][68/176]	Time 0.071 (0.080)	Data 0.001 (0.009)	Loss 0.4891 (0.6161)	Prec@1 94.531 (91.955)
Epoch: [60][85/176]	Time 0.071 (0.078)	Data 0.001 (0.008)	Loss 0.4954 (0.5927)	Prec@1 94.922 (92.419)
Epoch: [60][102/176]	Time 0.071 (0.077)	Data 0.001 (0.007)	Loss 0.4735 (0.5794)	Prec@1 95.312 (92.612)
Epoch: [60][119/176]	Time 0.071 (0.076)	Data 0.001 (0.006)	Loss 0.4784 (0.5699)	Prec@1 93.750 (92.812)
Epoch: [60][136/176]	Time 0.071 (0.076)	Data 0.001 (0.005)	Loss 0.6064 (0.5575)	Prec@1 90.625 (93.014)
Epoch: [60][153/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.4338 (0.5491)	Prec@1 93.750 (93.085)
Epoch: [60][170/176]	Time 0.070 (0.075)	Data 0.000 (0.005)	Loss 0.3912 (0.5379)	Prec@1 97.656 (93.366)
 * Prec@1 84.460
 * Prec@1 84.540
 * Prec@1 83.560
 * Prec@1 84.500
 * Noisy Prec@1 84.20
	 * New best: 84.20000
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.546 (0.546)	Data 0.474 (0.474)	Loss 0.4959 (0.4959)	Prec@1 94.922 (94.922)
Epoch: [61][17/176]	Time 0.071 (0.098)	Data 0.001 (0.028)	Loss 0.4262 (0.4461)	Prec@1 96.094 (95.247)
Epoch: [61][34/176]	Time 0.071 (0.085)	Data 0.001 (0.015)	Loss 0.4142 (0.4372)	Prec@1 94.141 (95.179)
Epoch: [61][51/176]	Time 0.073 (0.081)	Data 0.002 (0.010)	Loss 0.4285 (0.4433)	Prec@1 95.703 (94.997)
Epoch: [61][68/176]	Time 0.074 (0.079)	Data 0.002 (0.008)	Loss 0.3684 (0.4424)	Prec@1 97.266 (95.080)
Epoch: [61][85/176]	Time 0.071 (0.077)	Data 0.001 (0.007)	Loss 0.4220 (0.4385)	Prec@1 94.922 (95.113)
Epoch: [61][102/176]	Time 0.071 (0.076)	Data 0.001 (0.006)	Loss 0.3693 (0.4351)	Prec@1 94.531 (95.055)
Epoch: [61][119/176]	Time 0.072 (0.076)	Data 0.002 (0.005)	Loss 0.4026 (0.4312)	Prec@1 94.922 (95.133)
Epoch: [61][136/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.4784 (0.4306)	Prec@1 92.969 (95.121)
Epoch: [61][153/176]	Time 0.072 (0.075)	Data 0.001 (0.004)	Loss 0.4581 (0.4322)	Prec@1 93.359 (95.110)
Epoch: [61][170/176]	Time 0.070 (0.074)	Data 0.000 (0.004)	Loss 0.3932 (0.4321)	Prec@1 95.312 (95.052)
 * Prec@1 86.860
 * Prec@1 85.940
 * Prec@1 84.580
 * Prec@1 86.240
 * Noisy Prec@1 85.59
	 * New best: 85.58667
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.558 (0.558)	Data 0.479 (0.479)	Loss 0.3730 (0.3730)	Prec@1 96.875 (96.875)
Epoch: [62][17/176]	Time 0.073 (0.101)	Data 0.003 (0.028)	Loss 0.3233 (0.4073)	Prec@1 96.875 (95.095)
Epoch: [62][34/176]	Time 0.076 (0.087)	Data 0.002 (0.015)	Loss 0.3661 (0.3978)	Prec@1 96.875 (95.368)
Epoch: [62][51/176]	Time 0.071 (0.082)	Data 0.001 (0.011)	Loss 0.4336 (0.4013)	Prec@1 95.703 (95.448)
Epoch: [62][68/176]	Time 0.071 (0.080)	Data 0.001 (0.008)	Loss 0.3571 (0.3980)	Prec@1 95.703 (95.471)
Epoch: [62][85/176]	Time 0.071 (0.078)	Data 0.001 (0.007)	Loss 0.5308 (0.3991)	Prec@1 92.969 (95.494)
Epoch: [62][102/176]	Time 0.072 (0.077)	Data 0.002 (0.006)	Loss 0.3412 (0.4032)	Prec@1 96.484 (95.354)
Epoch: [62][119/176]	Time 0.071 (0.076)	Data 0.001 (0.005)	Loss 0.3819 (0.4016)	Prec@1 94.531 (95.303)
Epoch: [62][136/176]	Time 0.071 (0.076)	Data 0.001 (0.005)	Loss 0.4027 (0.4037)	Prec@1 94.141 (95.221)
Epoch: [62][153/176]	Time 0.071 (0.075)	Data 0.001 (0.004)	Loss 0.3931 (0.4040)	Prec@1 94.922 (95.160)
Epoch: [62][170/176]	Time 0.070 (0.075)	Data 0.000 (0.004)	Loss 0.3658 (0.4052)	Prec@1 96.094 (95.116)
 * Prec@1 88.020
 * Prec@1 86.580
 * Prec@1 87.140
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 142, in main
    mean_prec1, std_prec1 = validate_noisy(val_loader, model, args, eta_inf=args.eta_train, eta_mode=args.eta_mode, n_inf=3)
  File "trainer_resnet.py", line 255, in validate_noisy
    accs[i] = validate(val_loader, model_noisy, args)
  File "trainer_resnet.py", line 272, in validate
    for i, (input, target) in enumerate(val_loader):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 435, in __next__
    data = self._next_data()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1085, in _next_data
    return self._process_data(data)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1109, in _process_data
    self._try_put_index()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1091, in _try_put_index
    index = self._next_index()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 427, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/sampler.py", line 227, in __iter__
    for idx in self.sampler:
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/sampler.py", line 145, in <genexpr>
    return (self.indices[i] for i in torch.randperm(len(self.indices), generator=self.generator))
KeyboardInterrupt
