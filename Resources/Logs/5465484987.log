Sender: LSF System <lsfadmin@zhcc003>
Subject: Job 768145: <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=3 -beta_robustness=0.1 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5465484987> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=3 -beta_robustness=0.1 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5465484987> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov  2 14:09:21 2021
Job was executed on host(s) <zhcc003>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov  2 14:09:22 2021
</u/jbu> was used as the home directory.
</u/jbu/Master-Thesis> was used as the working directory.
Started at Tue Nov  2 14:09:22 2021
Terminated at Tue Nov  2 14:13:14 2021
Results reported at Tue Nov  2 14:13:14 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=3 -beta_robustness=0.1 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5465484987
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   362.80 sec.
    Max Memory :                                 2033 MB
    Average Memory :                             1848.60 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                131
    Run time :                                   232 sec.
    Turnaround time :                            233 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Master-Thesis/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 93.900
 * Prec@1 19.920
 * Prec@1 20.820
 * Prec@1 18.920
 * Prec@1 20.440
 * Noisy Prec@1 20.06
After loading, with clipping@2.00 19.92 w/o 93.90 noisy: 20.06pm1.01
current lr 2.00000e-02
Epoch: [60][0/176]	Time 1.025 (1.025)	Data 0.483 (0.483)	Loss 1.5937 (1.5937)	Prec@1 83.984 (83.984)
Epoch: [60][17/176]	Time 0.594 (0.513)	Data 0.002 (0.028)	Loss 1.2834 (1.4526)	Prec@1 90.234 (91.428)
Epoch: [60][34/176]	Time 0.523 (0.549)	Data 0.002 (0.015)	Loss 1.0879 (1.3739)	Prec@1 90.625 (90.804)
Epoch: [60][51/176]	Time 0.490 (0.525)	Data 0.002 (0.011)	Loss 1.0622 (1.2854)	Prec@1 92.188 (91.016)
Epoch: [60][68/176]	Time 0.467 (0.524)	Data 0.001 (0.008)	Loss 1.0003 (1.2212)	Prec@1 91.797 (91.129)
Epoch: [60][85/176]	Time 0.544 (0.522)	Data 0.002 (0.007)	Loss 0.9546 (1.1716)	Prec@1 92.188 (91.397)
Epoch: [60][102/176]	Time 0.558 (0.527)	Data 0.001 (0.006)	Loss 0.9026 (1.1330)	Prec@1 96.094 (91.615)
Epoch: [60][119/176]	Time 0.467 (0.520)	Data 0.001 (0.005)	Loss 0.8565 (1.1014)	Prec@1 92.969 (91.836)
Epoch: [60][136/176]	Time 0.466 (0.516)	Data 0.001 (0.005)	Loss 0.9359 (1.0729)	Prec@1 92.188 (92.042)
Epoch: [60][153/176]	Time 0.504 (0.511)	Data 0.001 (0.004)	Loss 0.8402 (1.0507)	Prec@1 92.969 (92.119)
Epoch: [60][170/176]	Time 0.465 (0.508)	Data 0.000 (0.004)	Loss 0.8023 (1.0294)	Prec@1 92.969 (92.251)
 * Prec@1 89.200
 * Prec@1 88.940
 * Prec@1 88.980
 * Prec@1 88.900
 * Noisy Prec@1 88.94
	 * New best: 88.94000
current lr 2.00000e-02
Epoch: [61][0/176]	Time 1.055 (1.055)	Data 0.566 (0.566)	Loss 0.8983 (0.8983)	Prec@1 94.922 (94.922)
Epoch: [61][17/176]	Time 0.469 (0.507)	Data 0.001 (0.033)	Loss 0.8166 (0.8115)	Prec@1 94.141 (94.466)
Epoch: [61][34/176]	Time 0.467 (0.504)	Data 0.001 (0.017)	Loss 0.7454 (0.8120)	Prec@1 96.094 (94.174)
Epoch: [61][51/176]	Time 0.471 (0.510)	Data 0.001 (0.012)	Loss 0.7816 (0.8028)	Prec@1 94.922 (94.201)
Epoch: [61][68/176]	Time 0.486 (0.508)	Data 0.002 (0.009)	Loss 0.7484 (0.8015)	Prec@1 95.312 (93.948)
Epoch: [61][85/176]	Time 0.600 (0.521)	Data 0.002 (0.008)	Loss 0.7638 (0.7969)	Prec@1 93.750 (93.982)
Epoch: [61][102/176]	Time 0.462 (0.516)	Data 0.001 (0.007)	Loss 0.8023 (0.7934)	Prec@1 93.359 (93.868)
Epoch: [61][119/176]	Time 0.462 (0.509)	Data 0.001 (0.006)	Loss 0.7342 (0.7875)	Prec@1 92.969 (93.893)
Epoch: [61][136/176]	Time 0.463 (0.504)	Data 0.001 (0.005)	Loss 0.7155 (0.7828)	Prec@1 93.750 (93.955)
Epoch: [61][153/176]	Time 0.465 (0.500)	Data 0.001 (0.005)	Loss 0.7610 (0.7810)	Prec@1 94.141 (93.897)
Epoch: [61][170/176]	Time 0.496 (0.498)	Data 0.000 (0.005)	Loss 0.6637 (0.7781)	Prec@1 94.922 (93.878)
 * Prec@1 89.420
 * Prec@1 89.200
 * Prec@1 89.060
 * Prec@1 89.120
 * Noisy Prec@1 89.13
	 * New best: 89.12667
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.976 (0.976)	Data 0.489 (0.489)	Loss 0.7041 (0.7041)	Prec@1 95.703 (95.703)
Epoch: [62][17/176]	Time 0.463 (0.496)	Data 0.001 (0.028)	Loss 0.6501 (0.7472)	Prec@1 96.484 (93.381)
Epoch: [62][34/176]	Time 0.465 (0.482)	Data 0.001 (0.015)	Loss 0.7042 (0.7443)	Prec@1 96.094 (93.694)
Epoch: [62][51/176]	Time 0.463 (0.477)	Data 0.001 (0.011)	Loss 0.7545 (0.7492)	Prec@1 92.969 (93.750)
Epoch: [62][68/176]	Time 0.468 (0.476)	Data 0.001 (0.008)	Loss 0.6776 (0.7448)	Prec@1 95.312 (93.835)
Traceback (most recent call last):
  File "trainer_resnet.py", line 339, in <module>
    main()
  File "trainer_resnet.py", line 144, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 219, in train
    epoch=epoch - args.start_epoch
  File "/ibm/gpfs-homes/jbu/Master-Thesis/Losses/torch_loss.py", line 138, in compute_gradient_and_backward
    X
  File "/ibm/gpfs-homes/jbu/Master-Thesis/Losses/torch_loss.py", line 97, in _adversarial_loss
    loss_rob.backward()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/autograd/__init__.py", line 132, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt
