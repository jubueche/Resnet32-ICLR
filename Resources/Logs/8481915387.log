Sender: LSF System <lsfadmin@zhcc004>
Subject: Job 777161: <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.037 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=8481915387> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.037 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=8481915387> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:13 2021
Job was executed on host(s) <zhcc004>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:14 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:14 2021
Terminated at Tue Nov 23 12:50:28 2021
Results reported at Tue Nov 23 12:50:28 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.037 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=8481915387
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   237.30 sec.
    Max Memory :                                 2013 MB
    Average Memory :                             1562.46 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                133
    Run time :                                   75 sec.
    Turnaround time :                            75 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 94.280
 * Prec@1 15.320
 * Prec@1 15.780
 * Prec@1 14.500
 * Prec@1 16.080
 * Noisy Prec@1 15.45
After loading, with clipping@2.00 15.32 w/o 94.28 noisy: 15.45pm0.84
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.700 (0.700)	Data 0.614 (0.614)	Loss 0.8340 (0.8340)	Prec@1 87.109 (87.109)
Epoch: [60][17/176]	Time 0.071 (0.106)	Data 0.001 (0.035)	Loss 0.6727 (0.7702)	Prec@1 89.453 (90.907)
Epoch: [60][34/176]	Time 0.072 (0.089)	Data 0.002 (0.019)	Loss 0.5704 (0.7150)	Prec@1 90.625 (90.647)
Epoch: [60][51/176]	Time 0.071 (0.084)	Data 0.001 (0.013)	Loss 0.6142 (0.6679)	Prec@1 91.406 (90.903)
Epoch: [60][68/176]	Time 0.071 (0.082)	Data 0.001 (0.010)	Loss 0.5012 (0.6362)	Prec@1 94.531 (91.378)
Epoch: [60][85/176]	Time 0.086 (0.082)	Data 0.002 (0.009)	Loss 0.5129 (0.6146)	Prec@1 95.703 (91.856)
Epoch: [60][102/176]	Time 0.090 (0.082)	Data 0.002 (0.007)	Loss 0.4616 (0.6025)	Prec@1 96.094 (92.070)
Epoch: [60][119/176]	Time 0.099 (0.082)	Data 0.002 (0.007)	Loss 0.4554 (0.5884)	Prec@1 94.141 (92.223)
Epoch: [60][136/176]	Time 0.077 (0.081)	Data 0.002 (0.006)	Loss 0.5720 (0.5754)	Prec@1 92.969 (92.487)
Epoch: [60][153/176]	Time 0.071 (0.081)	Data 0.001 (0.005)	Loss 0.3977 (0.5658)	Prec@1 94.141 (92.624)
Epoch: [60][170/176]	Time 0.070 (0.081)	Data 0.000 (0.005)	Loss 0.3715 (0.5535)	Prec@1 96.875 (92.848)
 * Prec@1 86.400
 * Prec@1 86.360
 * Prec@1 85.780
 * Prec@1 85.680
 * Noisy Prec@1 85.94
	 * New best: 85.94000
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.633 (0.633)	Data 0.527 (0.527)	Loss 0.4692 (0.4692)	Prec@1 93.750 (93.750)
Epoch: [61][17/176]	Time 0.086 (0.114)	Data 0.002 (0.031)	Loss 0.4208 (0.4252)	Prec@1 94.922 (95.486)
Epoch: [61][34/176]	Time 0.082 (0.098)	Data 0.001 (0.017)	Loss 0.3810 (0.4245)	Prec@1 94.922 (95.312)
Epoch: [61][51/176]	Time 0.076 (0.092)	Data 0.002 (0.012)	Loss 0.3886 (0.4293)	Prec@1 97.266 (95.155)
Epoch: [61][68/176]	Time 0.097 (0.089)	Data 0.002 (0.009)	Loss 0.4202 (0.4347)	Prec@1 95.312 (94.854)
Epoch: [61][85/176]	Time 0.071 (0.086)	Data 0.001 (0.008)	Loss 0.3669 (0.4305)	Prec@1 96.484 (94.949)
Epoch: [61][102/176]	Time 0.071 (0.084)	Data 0.001 (0.007)	Loss 0.3998 (0.4299)	Prec@1 93.750 (94.926)
Epoch: [61][119/176]	Time 0.071 (0.083)	Data 0.001 (0.006)	Loss 0.3770 (0.4260)	Prec@1 96.094 (94.967)
Epoch: [61][136/176]	Time 0.071 (0.081)	Data 0.001 (0.005)	Loss 0.4292 (0.4226)	Prec@1 94.922 (95.053)
Epoch: [61][153/176]	Time 0.075 (0.080)	Data 0.002 (0.005)	Loss 0.4450 (0.4231)	Prec@1 92.969 (95.056)
Epoch: [61][170/176]	Time 0.070 (0.080)	Data 0.000 (0.005)	Loss 0.4279 (0.4220)	Prec@1 93.359 (95.050)
 * Prec@1 85.700
 * Prec@1 85.880
 * Prec@1 84.760
 * Prec@1 85.400
 * Noisy Prec@1 85.35
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.621 (0.621)	Data 0.548 (0.548)	Loss 0.3882 (0.3882)	Prec@1 94.531 (94.531)
Epoch: [62][17/176]	Time 0.071 (0.103)	Data 0.001 (0.032)	Loss 0.3280 (0.4060)	Prec@1 96.484 (94.900)
Epoch: [62][34/176]	Time 0.071 (0.088)	Data 0.001 (0.017)	Loss 0.3931 (0.3989)	Prec@1 95.703 (95.056)
Epoch: [62][51/176]	Time 0.071 (0.083)	Data 0.001 (0.012)	Loss 0.4167 (0.4058)	Prec@1 94.141 (95.050)
Epoch: [62][68/176]	Time 0.076 (0.080)	Data 0.001 (0.009)	Loss 0.3683 (0.4059)	Prec@1 95.312 (95.120)
Epoch: [62][85/176]	Time 0.071 (0.078)	Data 0.001 (0.008)	Loss 0.5761 (0.4055)	Prec@1 93.359 (95.231)
Epoch: [62][102/176]	Time 0.071 (0.077)	Data 0.001 (0.007)	Loss 0.3409 (0.4075)	Prec@1 95.703 (95.165)
Epoch: [62][119/176]	Time 0.071 (0.076)	Data 0.001 (0.006)	Loss 0.4102 (0.4029)	Prec@1 94.141 (95.166)
Epoch: [62][136/176]	Time 0.071 (0.076)	Data 0.001 (0.005)	Loss 0.3743 (0.4020)	Prec@1 95.312 (95.178)
Epoch: [62][153/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.4123 (0.4022)	Prec@1 95.703 (95.155)
Epoch: [62][170/176]	Time 0.070 (0.075)	Data 0.000 (0.005)	Loss 0.4306 (0.4046)	Prec@1 94.531 (95.098)
 * Prec@1 88.160
 * Prec@1 87.600
 * Prec@1 87.980
 * Prec@1 87.960
 * Noisy Prec@1 87.85
	 * New best: 87.84667
current lr 2.00000e-02
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 138, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 200, in train
    for i, (input, target) in enumerate(train_loader):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 352, in __iter__
    return self._get_iterator()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 294, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 827, in __init__
    self._reset(loader, first_iter=True)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 857, in _reset
    self._try_put_index()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1091, in _try_put_index
    index = self._next_index()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 427, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/sampler.py", line 227, in __iter__
    for idx in self.sampler:
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/sampler.py", line 145, in __iter__
    return (self.indices[i] for i in torch.randperm(len(self.indices), generator=self.generator))
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/tensor.py", line 594, in __iter__
    return iter(self.unbind(0))
KeyboardInterrupt
