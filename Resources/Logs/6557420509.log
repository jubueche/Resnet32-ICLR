Sender: LSF System <lsfadmin@zhcc007>
Subject: Job 777174: <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.056 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=6557420509> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.056 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=6557420509> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:16 2021
Job was executed on host(s) <zhcc007>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:17 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:17 2021
Terminated at Tue Nov 23 12:50:29 2021
Results reported at Tue Nov 23 12:50:29 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.056 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=6557420509
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   236.32 sec.
    Max Memory :                                 2061 MB
    Average Memory :                             1264.92 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                133
    Run time :                                   78 sec.
    Turnaround time :                            73 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 99.440
 * Prec@1 14.440
 * Prec@1 14.380
 * Prec@1 15.120
 * Prec@1 14.040
 * Noisy Prec@1 14.51
After loading, with clipping@2.00 14.44 w/o 99.44 noisy: 14.51pm0.55
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.599 (0.599)	Data 0.499 (0.499)	Loss 0.9046 (0.9046)	Prec@1 88.672 (88.672)
Epoch: [60][17/176]	Time 0.071 (0.105)	Data 0.001 (0.029)	Loss 0.7719 (0.8195)	Prec@1 87.891 (89.388)
Epoch: [60][34/176]	Time 0.071 (0.089)	Data 0.001 (0.016)	Loss 0.6711 (0.7643)	Prec@1 90.234 (88.817)
Epoch: [60][51/176]	Time 0.071 (0.083)	Data 0.001 (0.011)	Loss 0.5558 (0.7095)	Prec@1 93.359 (89.791)
Epoch: [60][68/176]	Time 0.072 (0.080)	Data 0.001 (0.008)	Loss 0.5380 (0.6720)	Prec@1 92.969 (90.500)
Epoch: [60][85/176]	Time 0.071 (0.079)	Data 0.001 (0.007)	Loss 0.6241 (0.6413)	Prec@1 91.797 (91.007)
Epoch: [60][102/176]	Time 0.071 (0.078)	Data 0.001 (0.006)	Loss 0.5310 (0.6235)	Prec@1 94.531 (91.251)
Epoch: [60][119/176]	Time 0.071 (0.077)	Data 0.001 (0.005)	Loss 0.5089 (0.6057)	Prec@1 94.141 (91.608)
Epoch: [60][136/176]	Time 0.072 (0.076)	Data 0.002 (0.005)	Loss 0.5571 (0.5924)	Prec@1 92.969 (91.877)
Epoch: [60][153/176]	Time 0.072 (0.076)	Data 0.002 (0.005)	Loss 0.3880 (0.5792)	Prec@1 95.312 (92.078)
Epoch: [60][170/176]	Time 0.070 (0.075)	Data 0.000 (0.004)	Loss 0.5767 (0.5687)	Prec@1 90.625 (92.283)
 * Prec@1 87.620
 * Prec@1 87.020
 * Prec@1 86.480
 * Prec@1 86.080
 * Noisy Prec@1 86.53
	 * New best: 86.52667
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.553 (0.553)	Data 0.471 (0.471)	Loss 0.4557 (0.4557)	Prec@1 92.578 (92.578)
Epoch: [61][17/176]	Time 0.076 (0.101)	Data 0.002 (0.028)	Loss 0.4759 (0.4620)	Prec@1 93.359 (93.989)
Epoch: [61][34/176]	Time 0.071 (0.088)	Data 0.001 (0.015)	Loss 0.3429 (0.4597)	Prec@1 97.656 (94.085)
Epoch: [61][51/176]	Time 0.071 (0.082)	Data 0.001 (0.010)	Loss 0.4091 (0.4540)	Prec@1 96.094 (94.096)
Epoch: [61][68/176]	Time 0.085 (0.081)	Data 0.002 (0.008)	Loss 0.4437 (0.4451)	Prec@1 96.094 (94.322)
Epoch: [61][85/176]	Time 0.071 (0.079)	Data 0.001 (0.007)	Loss 0.4392 (0.4434)	Prec@1 96.094 (94.418)
Epoch: [61][102/176]	Time 0.071 (0.078)	Data 0.001 (0.006)	Loss 0.4992 (0.4448)	Prec@1 91.797 (94.406)
Epoch: [61][119/176]	Time 0.071 (0.077)	Data 0.001 (0.005)	Loss 0.3993 (0.4436)	Prec@1 95.703 (94.401)
Epoch: [61][136/176]	Time 0.071 (0.076)	Data 0.001 (0.005)	Loss 0.5235 (0.4433)	Prec@1 92.969 (94.409)
Epoch: [61][153/176]	Time 0.071 (0.075)	Data 0.001 (0.004)	Loss 0.4005 (0.4421)	Prec@1 95.312 (94.382)
Epoch: [61][170/176]	Time 0.069 (0.075)	Data 0.000 (0.004)	Loss 0.3576 (0.4411)	Prec@1 96.875 (94.326)
 * Prec@1 87.520
 * Prec@1 87.680
 * Prec@1 87.600
 * Prec@1 86.800
 * Noisy Prec@1 87.36
	 * New best: 87.36001
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.593 (0.593)	Data 0.522 (0.522)	Loss 0.5005 (0.5005)	Prec@1 94.922 (94.922)
Epoch: [62][17/176]	Time 0.071 (0.100)	Data 0.001 (0.030)	Loss 0.4871 (0.4212)	Prec@1 93.359 (94.510)
Epoch: [62][34/176]	Time 0.071 (0.086)	Data 0.001 (0.016)	Loss 0.4172 (0.4261)	Prec@1 94.531 (94.509)
Epoch: [62][51/176]	Time 0.071 (0.081)	Data 0.001 (0.011)	Loss 0.5051 (0.4313)	Prec@1 91.797 (94.336)
Epoch: [62][68/176]	Time 0.071 (0.079)	Data 0.001 (0.009)	Loss 0.5048 (0.4286)	Prec@1 92.188 (94.390)
Epoch: [62][85/176]	Time 0.071 (0.077)	Data 0.001 (0.007)	Loss 0.3274 (0.4306)	Prec@1 96.875 (94.372)
Epoch: [62][102/176]	Time 0.071 (0.076)	Data 0.001 (0.006)	Loss 0.5071 (0.4277)	Prec@1 92.578 (94.399)
Epoch: [62][119/176]	Time 0.071 (0.076)	Data 0.001 (0.006)	Loss 0.3943 (0.4249)	Prec@1 96.094 (94.421)
Epoch: [62][136/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.4345 (0.4237)	Prec@1 95.312 (94.443)
Epoch: [62][153/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.3501 (0.4251)	Prec@1 95.703 (94.425)
Epoch: [62][170/176]	Time 0.069 (0.074)	Data 0.000 (0.004)	Loss 0.3984 (0.4254)	Prec@1 93.359 (94.385)
 * Prec@1 89.580
 * Prec@1 89.360
 * Prec@1 89.640
 * Prec@1 89.480
 * Noisy Prec@1 89.49
	 * New best: 89.49334
current lr 2.00000e-02
Epoch: [63][0/176]	Time 0.552 (0.552)	Data 0.475 (0.475)	Loss 0.4884 (0.4884)	Prec@1 95.312 (95.312)
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 138, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 219, in train
    clip_fn(model)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "trainer_resnet.py", line 123, in clip_weights
    clamp_val = float(args.clipping_alpha * torch.std(v.view(-1), dim=0))
KeyboardInterrupt
