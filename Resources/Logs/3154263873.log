Sender: LSF System <lsfadmin@zhcc006>
Subject: Job 777171: <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.037 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=3154263873> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.037 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=3154263873> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:15 2021
Job was executed on host(s) <zhcc006>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:16 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:16 2021
Terminated at Tue Nov 23 12:50:28 2021
Results reported at Tue Nov 23 12:50:28 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.037 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=3154263873
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   247.43 sec.
    Max Memory :                                 2108 MB
    Average Memory :                             1621.92 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                133
    Run time :                                   75 sec.
    Turnaround time :                            73 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 94.280
 * Prec@1 15.320
 * Prec@1 15.820
 * Prec@1 15.180
 * Prec@1 15.480
 * Noisy Prec@1 15.49
After loading, with clipping@2.00 15.32 w/o 94.28 noisy: 15.49pm0.32
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.673 (0.673)	Data 0.550 (0.550)	Loss 0.8340 (0.8340)	Prec@1 87.109 (87.109)
Epoch: [60][17/176]	Time 0.108 (0.116)	Data 0.002 (0.032)	Loss 0.6176 (0.7702)	Prec@1 90.625 (90.495)
Epoch: [60][34/176]	Time 0.079 (0.099)	Data 0.002 (0.017)	Loss 0.6281 (0.7176)	Prec@1 89.062 (90.603)
Epoch: [60][51/176]	Time 0.083 (0.094)	Data 0.002 (0.012)	Loss 0.5741 (0.6670)	Prec@1 93.359 (91.113)
Epoch: [60][68/176]	Time 0.084 (0.091)	Data 0.001 (0.010)	Loss 0.4117 (0.6220)	Prec@1 95.312 (91.763)
Epoch: [60][85/176]	Time 0.084 (0.089)	Data 0.002 (0.008)	Loss 0.5531 (0.5981)	Prec@1 91.406 (92.310)
Epoch: [60][102/176]	Time 0.084 (0.087)	Data 0.002 (0.007)	Loss 0.4531 (0.5860)	Prec@1 94.922 (92.514)
Epoch: [60][119/176]	Time 0.071 (0.086)	Data 0.001 (0.006)	Loss 0.5403 (0.5722)	Prec@1 93.359 (92.660)
Epoch: [60][136/176]	Time 0.095 (0.085)	Data 0.002 (0.006)	Loss 0.5518 (0.5627)	Prec@1 93.359 (92.815)
Epoch: [60][153/176]	Time 0.072 (0.085)	Data 0.001 (0.005)	Loss 0.4594 (0.5538)	Prec@1 94.922 (92.994)
Epoch: [60][170/176]	Time 0.070 (0.085)	Data 0.000 (0.005)	Loss 0.3568 (0.5437)	Prec@1 98.438 (93.211)
 * Prec@1 84.840
 * Prec@1 84.940
 * Prec@1 84.440
 * Prec@1 84.900
 * Noisy Prec@1 84.76
	 * New best: 84.76000
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.558 (0.558)	Data 0.469 (0.469)	Loss 0.5401 (0.5401)	Prec@1 93.359 (93.359)
Epoch: [61][17/176]	Time 0.083 (0.115)	Data 0.002 (0.028)	Loss 0.4350 (0.4388)	Prec@1 94.922 (95.117)
Epoch: [61][34/176]	Time 0.075 (0.101)	Data 0.001 (0.015)	Loss 0.3670 (0.4344)	Prec@1 94.141 (94.688)
Epoch: [61][51/176]	Time 0.072 (0.093)	Data 0.001 (0.011)	Loss 0.4399 (0.4356)	Prec@1 95.312 (94.794)
Epoch: [61][68/176]	Time 0.071 (0.089)	Data 0.001 (0.008)	Loss 0.4160 (0.4363)	Prec@1 94.531 (94.701)
Epoch: [61][85/176]	Time 0.071 (0.087)	Data 0.001 (0.007)	Loss 0.4438 (0.4334)	Prec@1 93.750 (94.831)
Epoch: [61][102/176]	Time 0.084 (0.085)	Data 0.002 (0.006)	Loss 0.3710 (0.4315)	Prec@1 96.094 (94.884)
Epoch: [61][119/176]	Time 0.075 (0.084)	Data 0.001 (0.005)	Loss 0.4438 (0.4290)	Prec@1 94.922 (94.951)
Epoch: [61][136/176]	Time 0.072 (0.083)	Data 0.001 (0.005)	Loss 0.4352 (0.4267)	Prec@1 95.312 (94.959)
Epoch: [61][153/176]	Time 0.084 (0.082)	Data 0.002 (0.005)	Loss 0.4733 (0.4292)	Prec@1 92.969 (94.922)
Epoch: [61][170/176]	Time 0.070 (0.082)	Data 0.000 (0.004)	Loss 0.3969 (0.4289)	Prec@1 94.531 (94.851)
 * Prec@1 86.260
 * Prec@1 86.360
 * Prec@1 86.040
 * Prec@1 85.800
 * Noisy Prec@1 86.07
	 * New best: 86.06667
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.627 (0.627)	Data 0.515 (0.515)	Loss 0.3327 (0.3327)	Prec@1 98.047 (98.047)
Epoch: [62][17/176]	Time 0.091 (0.107)	Data 0.002 (0.030)	Loss 0.3156 (0.3893)	Prec@1 97.266 (95.790)
Epoch: [62][34/176]	Time 0.073 (0.091)	Data 0.001 (0.016)	Loss 0.3700 (0.3921)	Prec@1 96.484 (95.592)
Epoch: [62][51/176]	Time 0.080 (0.086)	Data 0.002 (0.011)	Loss 0.4360 (0.4008)	Prec@1 96.094 (95.560)
Epoch: [62][68/176]	Time 0.075 (0.083)	Data 0.001 (0.009)	Loss 0.3219 (0.3968)	Prec@1 97.266 (95.596)
Epoch: [62][85/176]	Time 0.073 (0.081)	Data 0.001 (0.007)	Loss 0.4570 (0.3995)	Prec@1 95.312 (95.562)
Epoch: [62][102/176]	Time 0.071 (0.079)	Data 0.001 (0.006)	Loss 0.2971 (0.4042)	Prec@1 97.266 (95.396)
Epoch: [62][119/176]	Time 0.083 (0.079)	Data 0.002 (0.006)	Loss 0.3843 (0.4009)	Prec@1 94.141 (95.394)
Epoch: [62][136/176]	Time 0.078 (0.079)	Data 0.001 (0.005)	Loss 0.3737 (0.4022)	Prec@1 95.703 (95.315)
Epoch: [62][153/176]	Time 0.089 (0.080)	Data 0.002 (0.005)	Loss 0.4480 (0.4029)	Prec@1 94.922 (95.307)
Epoch: [62][170/176]	Time 0.070 (0.080)	Data 0.000 (0.005)	Loss 0.3903 (0.4021)	Prec@1 95.703 (95.290)
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 141, in main
    prec1 = validate(val_loader, model, args=args)
  File "trainer_resnet.py", line 272, in validate
    for i, (input, target) in enumerate(val_loader):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 435, in __next__
    data = self._next_data()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1068, in _next_data
    idx, data = self._get_data()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1024, in _get_data
    success, data = self._try_get_data()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 872, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/threading.py", line 300, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt
