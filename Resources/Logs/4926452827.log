Sender: LSF System <lsfadmin@zhcc008>
Subject: Job 777165: <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.075 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=4926452827> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.075 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=4926452827> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:14 2021
Job was executed on host(s) <zhcc008>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:15 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:15 2021
Terminated at Tue Nov 23 12:50:28 2021
Results reported at Tue Nov 23 12:50:28 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.075 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=4926452827
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   225.28 sec.
    Max Memory :                                 2014 MB
    Average Memory :                             1577.08 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                133
    Run time :                                   74 sec.
    Turnaround time :                            74 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 94.280
 * Prec@1 15.320
 * Prec@1 16.600
 * Prec@1 13.840
 * Prec@1 18.020
 * Noisy Prec@1 16.15
After loading, with clipping@2.00 15.32 w/o 94.28 noisy: 16.15pm2.13
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.642 (0.642)	Data 0.534 (0.534)	Loss 0.8340 (0.8340)	Prec@1 87.109 (87.109)
Epoch: [60][17/176]	Time 0.083 (0.110)	Data 0.002 (0.031)	Loss 0.6390 (0.7792)	Prec@1 89.062 (90.799)
Epoch: [60][34/176]	Time 0.071 (0.092)	Data 0.001 (0.017)	Loss 0.6003 (0.7243)	Prec@1 89.844 (90.603)
Epoch: [60][51/176]	Time 0.077 (0.086)	Data 0.001 (0.012)	Loss 0.5827 (0.6688)	Prec@1 91.406 (91.256)
Epoch: [60][68/176]	Time 0.071 (0.084)	Data 0.001 (0.009)	Loss 0.4645 (0.6279)	Prec@1 95.312 (91.814)
Epoch: [60][85/176]	Time 0.087 (0.082)	Data 0.002 (0.008)	Loss 0.4960 (0.6048)	Prec@1 94.141 (92.383)
Epoch: [60][102/176]	Time 0.089 (0.081)	Data 0.002 (0.007)	Loss 0.3992 (0.5889)	Prec@1 95.703 (92.559)
Epoch: [60][119/176]	Time 0.071 (0.080)	Data 0.001 (0.006)	Loss 0.5116 (0.5736)	Prec@1 93.359 (92.764)
Epoch: [60][136/176]	Time 0.083 (0.079)	Data 0.002 (0.005)	Loss 0.5436 (0.5643)	Prec@1 91.406 (92.860)
Epoch: [60][153/176]	Time 0.071 (0.078)	Data 0.001 (0.005)	Loss 0.4217 (0.5576)	Prec@1 94.922 (92.921)
Epoch: [60][170/176]	Time 0.070 (0.078)	Data 0.000 (0.005)	Loss 0.3973 (0.5464)	Prec@1 96.484 (93.092)
 * Prec@1 84.400
 * Prec@1 84.240
 * Prec@1 81.780
 * Prec@1 81.740
 * Noisy Prec@1 82.59
	 * New best: 82.58666
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.615 (0.615)	Data 0.528 (0.528)	Loss 0.4772 (0.4772)	Prec@1 95.703 (95.703)
Epoch: [61][17/176]	Time 0.071 (0.111)	Data 0.001 (0.031)	Loss 0.3679 (0.4237)	Prec@1 97.656 (95.464)
Epoch: [61][34/176]	Time 0.073 (0.094)	Data 0.001 (0.017)	Loss 0.4328 (0.4226)	Prec@1 94.141 (95.324)
Epoch: [61][51/176]	Time 0.084 (0.090)	Data 0.002 (0.012)	Loss 0.4432 (0.4242)	Prec@1 95.703 (95.365)
Epoch: [61][68/176]	Time 0.071 (0.087)	Data 0.001 (0.009)	Loss 0.3665 (0.4284)	Prec@1 97.656 (95.312)
Epoch: [61][85/176]	Time 0.082 (0.085)	Data 0.001 (0.008)	Loss 0.3941 (0.4263)	Prec@1 95.312 (95.381)
Epoch: [61][102/176]	Time 0.071 (0.083)	Data 0.001 (0.007)	Loss 0.4736 (0.4286)	Prec@1 92.969 (95.290)
Epoch: [61][119/176]	Time 0.071 (0.082)	Data 0.001 (0.006)	Loss 0.4123 (0.4269)	Prec@1 96.094 (95.215)
Epoch: [61][136/176]	Time 0.071 (0.080)	Data 0.001 (0.005)	Loss 0.4395 (0.4248)	Prec@1 94.531 (95.201)
Epoch: [61][153/176]	Time 0.071 (0.079)	Data 0.001 (0.005)	Loss 0.3995 (0.4264)	Prec@1 97.266 (95.153)
Epoch: [61][170/176]	Time 0.069 (0.079)	Data 0.000 (0.005)	Loss 0.3871 (0.4246)	Prec@1 94.922 (95.146)
 * Prec@1 87.460
 * Prec@1 85.220
 * Prec@1 83.400
 * Prec@1 85.400
 * Noisy Prec@1 84.67
	 * New best: 84.67333
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.523 (0.523)	Data 0.441 (0.441)	Loss 0.3866 (0.3866)	Prec@1 97.266 (97.266)
Epoch: [62][17/176]	Time 0.071 (0.099)	Data 0.002 (0.026)	Loss 0.3445 (0.4070)	Prec@1 96.875 (95.117)
Epoch: [62][34/176]	Time 0.071 (0.085)	Data 0.001 (0.014)	Loss 0.3581 (0.3984)	Prec@1 96.875 (95.424)
Epoch: [62][51/176]	Time 0.071 (0.081)	Data 0.001 (0.010)	Loss 0.4038 (0.4022)	Prec@1 96.094 (95.433)
Epoch: [62][68/176]	Time 0.071 (0.079)	Data 0.001 (0.008)	Loss 0.3754 (0.4021)	Prec@1 93.750 (95.443)
Epoch: [62][85/176]	Time 0.071 (0.078)	Data 0.001 (0.006)	Loss 0.5201 (0.4018)	Prec@1 92.969 (95.449)
Epoch: [62][102/176]	Time 0.071 (0.077)	Data 0.001 (0.006)	Loss 0.3876 (0.4058)	Prec@1 96.484 (95.354)
Epoch: [62][119/176]	Time 0.071 (0.076)	Data 0.001 (0.005)	Loss 0.3861 (0.4038)	Prec@1 94.141 (95.374)
Epoch: [62][136/176]	Time 0.071 (0.076)	Data 0.001 (0.004)	Loss 0.4521 (0.4080)	Prec@1 92.188 (95.247)
Epoch: [62][153/176]	Time 0.071 (0.075)	Data 0.001 (0.004)	Loss 0.4507 (0.4092)	Prec@1 94.531 (95.186)
Epoch: [62][170/176]	Time 0.069 (0.075)	Data 0.000 (0.004)	Loss 0.3593 (0.4094)	Prec@1 95.703 (95.175)
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 138, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 213, in train
    epoch=epoch - args.start_epoch
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Losses/torch_loss.py", line 153, in compute_gradient_and_backward
    nat_loss = self.natural_loss(model(X), y)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 159, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Architectures/cifar_resnet.py", line 108, in forward
    out = self.layer1(out)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Architectures/cifar_resnet.py", line 76, in forward
    out = F.relu(self.bn1(self.conv1(x)))
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 136, in forward
    self.weight, self.bias, bn_training, exponential_average_factor, self.eps)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/functional.py", line 2058, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
KeyboardInterrupt
