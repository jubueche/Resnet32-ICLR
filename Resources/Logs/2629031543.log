Sender: LSF System <lsfadmin@zhcc006>
Subject: Job 777170: <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.026 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=2629031543> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.026 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=2629031543> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:15 2021
Job was executed on host(s) <zhcc006>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:16 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:16 2021
Terminated at Tue Nov 23 12:50:29 2021
Results reported at Tue Nov 23 12:50:29 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.026 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=2629031543
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   244.02 sec.
    Max Memory :                                 2053 MB
    Average Memory :                             1378.31 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                133
    Run time :                                   75 sec.
    Turnaround time :                            74 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 99.440
 * Prec@1 14.440
 * Prec@1 14.240
 * Prec@1 14.560
 * Prec@1 14.180
 * Noisy Prec@1 14.33
After loading, with clipping@2.00 14.44 w/o 99.44 noisy: 14.33pm0.20
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.577 (0.577)	Data 0.460 (0.460)	Loss 0.9046 (0.9046)	Prec@1 88.672 (88.672)
Epoch: [60][17/176]	Time 0.088 (0.115)	Data 0.002 (0.027)	Loss 0.7512 (0.8047)	Prec@1 86.719 (89.323)
Epoch: [60][34/176]	Time 0.079 (0.096)	Data 0.002 (0.015)	Loss 0.6572 (0.7542)	Prec@1 91.016 (89.152)
Epoch: [60][51/176]	Time 0.072 (0.090)	Data 0.001 (0.010)	Loss 0.5815 (0.7096)	Prec@1 91.406 (89.926)
Epoch: [60][68/176]	Time 0.093 (0.089)	Data 0.002 (0.008)	Loss 0.4934 (0.6695)	Prec@1 95.703 (90.523)
Epoch: [60][85/176]	Time 0.081 (0.086)	Data 0.002 (0.007)	Loss 0.5633 (0.6370)	Prec@1 95.312 (91.188)
Epoch: [60][102/176]	Time 0.082 (0.085)	Data 0.002 (0.006)	Loss 0.5124 (0.6183)	Prec@1 95.312 (91.478)
Epoch: [60][119/176]	Time 0.079 (0.085)	Data 0.002 (0.005)	Loss 0.4682 (0.5998)	Prec@1 95.312 (91.855)
Epoch: [60][136/176]	Time 0.073 (0.084)	Data 0.002 (0.005)	Loss 0.5696 (0.5874)	Prec@1 94.531 (92.145)
Epoch: [60][153/176]	Time 0.083 (0.084)	Data 0.002 (0.005)	Loss 0.4041 (0.5773)	Prec@1 95.312 (92.291)
Epoch: [60][170/176]	Time 0.073 (0.084)	Data 0.000 (0.004)	Loss 0.5292 (0.5670)	Prec@1 92.188 (92.407)
 * Prec@1 84.360
 * Prec@1 84.480
 * Prec@1 83.240
 * Prec@1 83.880
 * Noisy Prec@1 83.87
	 * New best: 83.86667
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.505 (0.505)	Data 0.433 (0.433)	Loss 0.4730 (0.4730)	Prec@1 93.750 (93.750)
Epoch: [61][17/176]	Time 0.072 (0.102)	Data 0.001 (0.025)	Loss 0.4706 (0.4625)	Prec@1 93.359 (94.184)
Epoch: [61][34/176]	Time 0.074 (0.091)	Data 0.001 (0.014)	Loss 0.3382 (0.4554)	Prec@1 98.828 (94.420)
Epoch: [61][51/176]	Time 0.095 (0.088)	Data 0.002 (0.010)	Loss 0.3599 (0.4513)	Prec@1 96.484 (94.411)
Epoch: [61][68/176]	Time 0.086 (0.087)	Data 0.002 (0.008)	Loss 0.4958 (0.4478)	Prec@1 92.969 (94.435)
Epoch: [61][85/176]	Time 0.085 (0.085)	Data 0.002 (0.007)	Loss 0.4785 (0.4511)	Prec@1 92.969 (94.304)
Epoch: [61][102/176]	Time 0.071 (0.085)	Data 0.001 (0.006)	Loss 0.4280 (0.4533)	Prec@1 93.359 (94.167)
Epoch: [61][119/176]	Time 0.071 (0.084)	Data 0.001 (0.005)	Loss 0.4261 (0.4523)	Prec@1 94.141 (94.199)
Epoch: [61][136/176]	Time 0.073 (0.084)	Data 0.002 (0.005)	Loss 0.4751 (0.4487)	Prec@1 91.406 (94.286)
Epoch: [61][153/176]	Time 0.077 (0.084)	Data 0.002 (0.004)	Loss 0.4099 (0.4464)	Prec@1 95.312 (94.303)
Epoch: [61][170/176]	Time 0.070 (0.083)	Data 0.000 (0.004)	Loss 0.3780 (0.4448)	Prec@1 96.484 (94.337)
 * Prec@1 88.240
 * Prec@1 87.820
 * Prec@1 88.100
 * Prec@1 88.000
 * Noisy Prec@1 87.97
	 * New best: 87.97333
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.644 (0.644)	Data 0.569 (0.569)	Loss 0.5381 (0.5381)	Prec@1 94.531 (94.531)
Epoch: [62][17/176]	Time 0.072 (0.108)	Data 0.001 (0.033)	Loss 0.4558 (0.4198)	Prec@1 92.188 (94.336)
Epoch: [62][34/176]	Time 0.074 (0.096)	Data 0.001 (0.018)	Loss 0.4184 (0.4185)	Prec@1 94.922 (94.632)
Epoch: [62][51/176]	Time 0.085 (0.092)	Data 0.002 (0.012)	Loss 0.4918 (0.4225)	Prec@1 92.188 (94.629)
Epoch: [62][68/176]	Time 0.071 (0.089)	Data 0.001 (0.010)	Loss 0.4464 (0.4195)	Prec@1 92.578 (94.667)
Epoch: [62][85/176]	Time 0.083 (0.089)	Data 0.002 (0.008)	Loss 0.3862 (0.4234)	Prec@1 96.094 (94.645)
Epoch: [62][102/176]	Time 0.073 (0.087)	Data 0.001 (0.007)	Loss 0.4496 (0.4220)	Prec@1 91.406 (94.653)
Epoch: [62][119/176]	Time 0.071 (0.086)	Data 0.001 (0.006)	Loss 0.3857 (0.4197)	Prec@1 95.703 (94.606)
Epoch: [62][136/176]	Time 0.078 (0.085)	Data 0.002 (0.006)	Loss 0.4282 (0.4211)	Prec@1 93.750 (94.571)
Epoch: [62][153/176]	Time 0.071 (0.084)	Data 0.001 (0.005)	Loss 0.3818 (0.4203)	Prec@1 95.703 (94.661)
Epoch: [62][170/176]	Time 0.071 (0.083)	Data 0.000 (0.005)	Loss 0.3532 (0.4178)	Prec@1 97.266 (94.709)
Traceback (most recent call last):
  File "trainer_resnet.py", line 222, in train
    output = model(input_var) #! This is inefficient, right?
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 151, in forward
    for t in chain(self.module.parameters(), self.module.buffers()):
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 138, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 222, in train
    output = model(input_var) #! This is inefficient, right?
KeyboardInterrupt
