Sender: LSF System <lsfadmin@zhcc007>
Subject: Job 777173: <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.056 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5485888202> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.056 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5485888202> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:16 2021
Job was executed on host(s) <zhcc007>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:17 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:17 2021
Terminated at Tue Nov 23 12:50:29 2021
Results reported at Tue Nov 23 12:50:29 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.056 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5485888202
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   235.84 sec.
    Max Memory :                                 2031 MB
    Average Memory :                             1271.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                133
    Run time :                                   78 sec.
    Turnaround time :                            73 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 94.280
 * Prec@1 15.320
 * Prec@1 16.120
 * Prec@1 15.120
 * Prec@1 15.660
 * Noisy Prec@1 15.63
After loading, with clipping@2.00 15.32 w/o 94.28 noisy: 15.63pm0.50
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.572 (0.572)	Data 0.474 (0.474)	Loss 0.8340 (0.8340)	Prec@1 87.109 (87.109)
Epoch: [60][17/176]	Time 0.078 (0.105)	Data 0.001 (0.028)	Loss 0.6692 (0.7651)	Prec@1 88.672 (90.907)
Epoch: [60][34/176]	Time 0.071 (0.090)	Data 0.001 (0.015)	Loss 0.5991 (0.7161)	Prec@1 90.625 (90.592)
Epoch: [60][51/176]	Time 0.071 (0.084)	Data 0.001 (0.010)	Loss 0.5232 (0.6534)	Prec@1 94.141 (91.346)
Epoch: [60][68/176]	Time 0.072 (0.081)	Data 0.001 (0.008)	Loss 0.4078 (0.6115)	Prec@1 96.875 (92.125)
Epoch: [60][85/176]	Time 0.071 (0.079)	Data 0.001 (0.007)	Loss 0.5201 (0.5909)	Prec@1 94.141 (92.592)
Epoch: [60][102/176]	Time 0.071 (0.078)	Data 0.001 (0.006)	Loss 0.5307 (0.5755)	Prec@1 94.141 (92.825)
Epoch: [60][119/176]	Time 0.075 (0.077)	Data 0.001 (0.005)	Loss 0.5666 (0.5644)	Prec@1 92.969 (92.959)
Epoch: [60][136/176]	Time 0.071 (0.076)	Data 0.002 (0.005)	Loss 0.5568 (0.5565)	Prec@1 93.359 (93.117)
Epoch: [60][153/176]	Time 0.071 (0.076)	Data 0.001 (0.004)	Loss 0.3759 (0.5497)	Prec@1 94.531 (93.192)
Epoch: [60][170/176]	Time 0.070 (0.076)	Data 0.000 (0.004)	Loss 0.4145 (0.5393)	Prec@1 96.875 (93.364)
 * Prec@1 85.400
 * Prec@1 84.940
 * Prec@1 84.860
 * Prec@1 85.260
 * Noisy Prec@1 85.02
	 * New best: 85.02000
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.527 (0.527)	Data 0.452 (0.452)	Loss 0.4905 (0.4905)	Prec@1 94.141 (94.141)
Epoch: [61][17/176]	Time 0.072 (0.098)	Data 0.001 (0.026)	Loss 0.3754 (0.4344)	Prec@1 98.438 (95.812)
Epoch: [61][34/176]	Time 0.071 (0.085)	Data 0.001 (0.014)	Loss 0.3660 (0.4302)	Prec@1 95.703 (95.513)
Epoch: [61][51/176]	Time 0.074 (0.081)	Data 0.001 (0.010)	Loss 0.4854 (0.4443)	Prec@1 93.750 (95.252)
Epoch: [61][68/176]	Time 0.071 (0.079)	Data 0.001 (0.008)	Loss 0.4225 (0.4446)	Prec@1 96.875 (95.012)
Epoch: [61][85/176]	Time 0.071 (0.078)	Data 0.001 (0.007)	Loss 0.4132 (0.4387)	Prec@1 96.094 (95.085)
Epoch: [61][102/176]	Time 0.074 (0.077)	Data 0.002 (0.006)	Loss 0.3922 (0.4375)	Prec@1 95.312 (95.047)
Epoch: [61][119/176]	Time 0.072 (0.077)	Data 0.001 (0.005)	Loss 0.3710 (0.4311)	Prec@1 96.484 (95.120)
Epoch: [61][136/176]	Time 0.071 (0.076)	Data 0.001 (0.005)	Loss 0.4953 (0.4294)	Prec@1 93.750 (95.150)
Epoch: [61][153/176]	Time 0.071 (0.076)	Data 0.001 (0.004)	Loss 0.4187 (0.4319)	Prec@1 94.922 (95.051)
Epoch: [61][170/176]	Time 0.070 (0.076)	Data 0.000 (0.004)	Loss 0.3718 (0.4302)	Prec@1 95.312 (95.054)
 * Prec@1 85.900
 * Prec@1 86.120
 * Prec@1 84.820
 * Prec@1 86.040
 * Noisy Prec@1 85.66
	 * New best: 85.66000
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.545 (0.545)	Data 0.471 (0.471)	Loss 0.3850 (0.3850)	Prec@1 96.094 (96.094)
Epoch: [62][17/176]	Time 0.082 (0.100)	Data 0.003 (0.028)	Loss 0.3144 (0.4059)	Prec@1 97.656 (95.139)
Epoch: [62][34/176]	Time 0.072 (0.087)	Data 0.002 (0.015)	Loss 0.3659 (0.4063)	Prec@1 96.094 (94.978)
Epoch: [62][51/176]	Time 0.071 (0.082)	Data 0.001 (0.010)	Loss 0.3716 (0.4128)	Prec@1 96.484 (94.832)
Epoch: [62][68/176]	Time 0.071 (0.080)	Data 0.001 (0.008)	Loss 0.3726 (0.4069)	Prec@1 94.531 (95.012)
Epoch: [62][85/176]	Time 0.074 (0.079)	Data 0.002 (0.007)	Loss 0.4883 (0.4072)	Prec@1 94.141 (95.117)
Epoch: [62][102/176]	Time 0.084 (0.079)	Data 0.003 (0.006)	Loss 0.3539 (0.4089)	Prec@1 95.312 (95.036)
Epoch: [62][119/176]	Time 0.071 (0.078)	Data 0.001 (0.005)	Loss 0.4120 (0.4067)	Prec@1 95.703 (95.117)
Epoch: [62][136/176]	Time 0.072 (0.077)	Data 0.001 (0.005)	Loss 0.3681 (0.4067)	Prec@1 94.922 (95.116)
Epoch: [62][153/176]	Time 0.072 (0.077)	Data 0.003 (0.004)	Loss 0.4675 (0.4060)	Prec@1 93.750 (95.115)
Epoch: [62][170/176]	Time 0.070 (0.077)	Data 0.000 (0.004)	Loss 0.3784 (0.4068)	Prec@1 96.875 (95.066)
 * Prec@1 86.080
 * Prec@1 86.340
 * Prec@1 85.480
 * Prec@1 85.200
 * Noisy Prec@1 85.67
	 * New best: 85.67334
current lr 2.00000e-02
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1002857e24d0>
Traceback (most recent call last):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1202, in __del__
    def __del__(self):
KeyboardInterrupt
Epoch: [63][0/176]	Time 0.561 (0.561)	Data 0.484 (0.484)	Loss 0.4258 (0.4258)	Prec@1 94.141 (94.141)
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 138, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 213, in train
    epoch=epoch - args.start_epoch
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Losses/torch_loss.py", line 153, in compute_gradient_and_backward
    nat_loss = self.natural_loss(model(X), y)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 159, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Architectures/cifar_resnet.py", line 109, in forward
    out = self.layer2(out)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Architectures/cifar_resnet.py", line 77, in forward
    out = self.bn2(self.conv2(out))
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 111, in forward
    self.num_batches_tracked = self.num_batches_tracked + 1
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 824, in __setattr__
    buffers[name] = value
KeyboardInterrupt
