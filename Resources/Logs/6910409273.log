Sender: LSF System <lsfadmin@zhcc011>
Subject: Job 777169: <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.026 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=6910409273> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.026 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=6910409273> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:15 2021
Job was executed on host(s) <zhcc011>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:16 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:16 2021
Terminated at Tue Nov 23 12:50:28 2021
Results reported at Tue Nov 23 12:50:28 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.026 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=6910409273
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   216.41 sec.
    Max Memory :                                 2014 MB
    Average Memory :                             1590.15 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                131
    Run time :                                   75 sec.
    Turnaround time :                            73 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 94.280
 * Prec@1 15.320
 * Prec@1 15.740
 * Prec@1 15.220
 * Prec@1 15.400
 * Noisy Prec@1 15.45
After loading, with clipping@2.00 15.32 w/o 94.28 noisy: 15.45pm0.26
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.561 (0.561)	Data 0.449 (0.449)	Loss 0.8340 (0.8340)	Prec@1 87.109 (87.109)
Epoch: [60][17/176]	Time 0.073 (0.098)	Data 0.001 (0.026)	Loss 0.6527 (0.7715)	Prec@1 91.016 (91.254)
Epoch: [60][34/176]	Time 0.070 (0.086)	Data 0.001 (0.014)	Loss 0.6372 (0.7144)	Prec@1 89.844 (90.592)
Epoch: [60][51/176]	Time 0.072 (0.081)	Data 0.002 (0.010)	Loss 0.5646 (0.6563)	Prec@1 92.969 (91.602)
Epoch: [60][68/176]	Time 0.071 (0.079)	Data 0.001 (0.008)	Loss 0.4553 (0.6155)	Prec@1 94.922 (92.250)
Epoch: [60][85/176]	Time 0.071 (0.077)	Data 0.001 (0.007)	Loss 0.4644 (0.5898)	Prec@1 97.266 (92.778)
Epoch: [60][102/176]	Time 0.071 (0.076)	Data 0.001 (0.006)	Loss 0.5157 (0.5803)	Prec@1 95.312 (92.954)
Epoch: [60][119/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.5157 (0.5677)	Prec@1 94.141 (93.132)
Epoch: [60][136/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.5628 (0.5573)	Prec@1 92.969 (93.223)
Epoch: [60][153/176]	Time 0.071 (0.074)	Data 0.001 (0.004)	Loss 0.4203 (0.5494)	Prec@1 94.922 (93.344)
Epoch: [60][170/176]	Time 0.069 (0.074)	Data 0.000 (0.004)	Loss 0.3908 (0.5411)	Prec@1 96.484 (93.432)
 * Prec@1 85.540
 * Prec@1 85.320
 * Prec@1 85.080
 * Prec@1 85.300
 * Noisy Prec@1 85.23
	 * New best: 85.23333
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.504 (0.504)	Data 0.433 (0.433)	Loss 0.4727 (0.4727)	Prec@1 95.312 (95.312)
Epoch: [61][17/176]	Time 0.073 (0.095)	Data 0.002 (0.025)	Loss 0.4049 (0.4321)	Prec@1 96.094 (95.530)
Epoch: [61][34/176]	Time 0.071 (0.084)	Data 0.001 (0.014)	Loss 0.4224 (0.4373)	Prec@1 93.750 (95.022)
Epoch: [61][51/176]	Time 0.071 (0.079)	Data 0.001 (0.010)	Loss 0.4365 (0.4358)	Prec@1 94.922 (95.125)
Epoch: [61][68/176]	Time 0.071 (0.077)	Data 0.001 (0.007)	Loss 0.3726 (0.4361)	Prec@1 96.094 (95.046)
Epoch: [61][85/176]	Time 0.071 (0.076)	Data 0.001 (0.006)	Loss 0.4455 (0.4344)	Prec@1 91.797 (95.040)
Epoch: [61][102/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.4152 (0.4334)	Prec@1 94.922 (94.975)
Epoch: [61][119/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.3661 (0.4308)	Prec@1 95.703 (95.000)
Epoch: [61][136/176]	Time 0.071 (0.074)	Data 0.001 (0.004)	Loss 0.4479 (0.4291)	Prec@1 95.312 (95.067)
Epoch: [61][153/176]	Time 0.071 (0.074)	Data 0.001 (0.004)	Loss 0.4099 (0.4308)	Prec@1 94.922 (94.968)
Epoch: [61][170/176]	Time 0.070 (0.074)	Data 0.000 (0.004)	Loss 0.4035 (0.4292)	Prec@1 94.141 (94.974)
 * Prec@1 87.100
 * Prec@1 86.780
 * Prec@1 86.600
 * Prec@1 87.080
 * Noisy Prec@1 86.82
	 * New best: 86.82001
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.491 (0.491)	Data 0.413 (0.413)	Loss 0.3628 (0.3628)	Prec@1 96.484 (96.484)
Epoch: [62][17/176]	Time 0.071 (0.095)	Data 0.001 (0.024)	Loss 0.3244 (0.4107)	Prec@1 96.484 (95.095)
Epoch: [62][34/176]	Time 0.071 (0.084)	Data 0.001 (0.013)	Loss 0.4177 (0.4051)	Prec@1 97.266 (95.357)
Epoch: [62][51/176]	Time 0.071 (0.079)	Data 0.001 (0.009)	Loss 0.3834 (0.4087)	Prec@1 96.094 (95.312)
Epoch: [62][68/176]	Time 0.071 (0.077)	Data 0.001 (0.007)	Loss 0.4004 (0.4068)	Prec@1 92.578 (95.194)
Epoch: [62][85/176]	Time 0.071 (0.076)	Data 0.001 (0.006)	Loss 0.4973 (0.4091)	Prec@1 93.359 (95.217)
Epoch: [62][102/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.3751 (0.4138)	Prec@1 95.312 (95.051)
Epoch: [62][119/176]	Time 0.075 (0.075)	Data 0.001 (0.005)	Loss 0.4081 (0.4102)	Prec@1 94.531 (95.075)
Epoch: [62][136/176]	Time 0.071 (0.074)	Data 0.001 (0.004)	Loss 0.4109 (0.4112)	Prec@1 93.359 (95.030)
Epoch: [62][153/176]	Time 0.071 (0.074)	Data 0.001 (0.004)	Loss 0.3986 (0.4118)	Prec@1 95.312 (94.952)
Epoch: [62][170/176]	Time 0.070 (0.074)	Data 0.000 (0.004)	Loss 0.3675 (0.4114)	Prec@1 96.094 (94.924)
 * Prec@1 86.080
 * Prec@1 86.120
 * Prec@1 85.940
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 142, in main
    mean_prec1, std_prec1 = validate_noisy(val_loader, model, args, eta_inf=args.eta_train, eta_mode=args.eta_mode, n_inf=3)
  File "trainer_resnet.py", line 255, in validate_noisy
    accs[i] = validate(val_loader, model_noisy, args)
  File "trainer_resnet.py", line 284, in validate
    top1.update(prec1.item(), input.size(0))
KeyboardInterrupt
