Sender: LSF System <lsfadmin@zhcc004>
Subject: Job 777162: <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.037 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5572698474> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.037 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5572698474> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:13 2021
Job was executed on host(s) <zhcc004>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:14 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:14 2021
Terminated at Tue Nov 23 12:50:30 2021
Results reported at Tue Nov 23 12:50:30 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.037 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5572698474
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   242.71 sec.
    Max Memory :                                 2074 MB
    Average Memory :                             1365.54 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                133
    Run time :                                   75 sec.
    Turnaround time :                            77 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 99.440
 * Prec@1 14.440
 * Prec@1 14.140
 * Prec@1 13.720
 * Prec@1 14.740
 * Noisy Prec@1 14.20
After loading, with clipping@2.00 14.44 w/o 99.44 noisy: 14.20pm0.51
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.646 (0.646)	Data 0.540 (0.540)	Loss 0.9046 (0.9046)	Prec@1 88.672 (88.672)
Epoch: [60][17/176]	Time 0.099 (0.112)	Data 0.001 (0.031)	Loss 0.7644 (0.8053)	Prec@1 88.672 (90.343)
Epoch: [60][34/176]	Time 0.088 (0.099)	Data 0.002 (0.017)	Loss 0.7322 (0.7664)	Prec@1 91.016 (89.542)
Epoch: [60][51/176]	Time 0.086 (0.094)	Data 0.001 (0.012)	Loss 0.5632 (0.7226)	Prec@1 93.359 (89.971)
Epoch: [60][68/176]	Time 0.086 (0.091)	Data 0.002 (0.009)	Loss 0.5031 (0.6761)	Prec@1 94.141 (90.744)
Epoch: [60][85/176]	Time 0.084 (0.090)	Data 0.002 (0.008)	Loss 0.6042 (0.6424)	Prec@1 91.406 (91.397)
Epoch: [60][102/176]	Time 0.086 (0.088)	Data 0.002 (0.007)	Loss 0.5129 (0.6209)	Prec@1 94.531 (91.748)
Epoch: [60][119/176]	Time 0.086 (0.088)	Data 0.002 (0.006)	Loss 0.4440 (0.6024)	Prec@1 95.312 (92.106)
Epoch: [60][136/176]	Time 0.085 (0.087)	Data 0.002 (0.006)	Loss 0.5052 (0.5894)	Prec@1 95.312 (92.367)
Epoch: [60][153/176]	Time 0.071 (0.085)	Data 0.001 (0.005)	Loss 0.4544 (0.5758)	Prec@1 96.484 (92.510)
Epoch: [60][170/176]	Time 0.069 (0.084)	Data 0.000 (0.005)	Loss 0.5811 (0.5683)	Prec@1 91.406 (92.596)
 * Prec@1 86.160
 * Prec@1 86.820
 * Prec@1 85.160
 * Prec@1 84.180
 * Noisy Prec@1 85.39
	 * New best: 85.38667
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.672 (0.672)	Data 0.580 (0.580)	Loss 0.5268 (0.5268)	Prec@1 91.016 (91.016)
Epoch: [61][17/176]	Time 0.074 (0.120)	Data 0.001 (0.035)	Loss 0.4594 (0.4661)	Prec@1 94.922 (93.924)
Epoch: [61][34/176]	Time 0.082 (0.100)	Data 0.002 (0.019)	Loss 0.4079 (0.4671)	Prec@1 96.094 (94.163)
Epoch: [61][51/176]	Time 0.071 (0.093)	Data 0.001 (0.013)	Loss 0.4115 (0.4623)	Prec@1 94.922 (94.028)
Epoch: [61][68/176]	Time 0.082 (0.089)	Data 0.001 (0.010)	Loss 0.4326 (0.4585)	Prec@1 96.484 (94.135)
Epoch: [61][85/176]	Time 0.076 (0.087)	Data 0.002 (0.009)	Loss 0.4535 (0.4557)	Prec@1 94.531 (94.245)
Epoch: [61][102/176]	Time 0.073 (0.084)	Data 0.001 (0.007)	Loss 0.4365 (0.4546)	Prec@1 94.141 (94.205)
Epoch: [61][119/176]	Time 0.071 (0.083)	Data 0.001 (0.006)	Loss 0.3979 (0.4517)	Prec@1 94.922 (94.222)
Epoch: [61][136/176]	Time 0.072 (0.081)	Data 0.001 (0.006)	Loss 0.4930 (0.4480)	Prec@1 92.969 (94.292)
Epoch: [61][153/176]	Time 0.071 (0.080)	Data 0.001 (0.005)	Loss 0.5037 (0.4459)	Prec@1 92.578 (94.283)
Epoch: [61][170/176]	Time 0.069 (0.080)	Data 0.000 (0.005)	Loss 0.3795 (0.4448)	Prec@1 94.922 (94.289)
 * Prec@1 89.580
 * Prec@1 88.460
 * Prec@1 89.020
 * Prec@1 89.320
 * Noisy Prec@1 88.93
	 * New best: 88.93333
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.534 (0.534)	Data 0.449 (0.449)	Loss 0.5087 (0.5087)	Prec@1 93.750 (93.750)
Epoch: [62][17/176]	Time 0.071 (0.098)	Data 0.001 (0.026)	Loss 0.4270 (0.4025)	Prec@1 94.141 (94.987)
Epoch: [62][34/176]	Time 0.080 (0.090)	Data 0.001 (0.014)	Loss 0.4198 (0.4103)	Prec@1 94.922 (95.134)
Epoch: [62][51/176]	Time 0.082 (0.086)	Data 0.002 (0.010)	Loss 0.4945 (0.4215)	Prec@1 94.141 (94.974)
Epoch: [62][68/176]	Time 0.078 (0.083)	Data 0.002 (0.008)	Loss 0.4243 (0.4215)	Prec@1 94.922 (94.945)
Epoch: [62][85/176]	Time 0.081 (0.081)	Data 0.003 (0.007)	Loss 0.4241 (0.4238)	Prec@1 96.094 (94.904)
Epoch: [62][102/176]	Time 0.071 (0.080)	Data 0.001 (0.006)	Loss 0.4643 (0.4214)	Prec@1 91.016 (94.876)
Epoch: [62][119/176]	Time 0.071 (0.079)	Data 0.001 (0.005)	Loss 0.4155 (0.4198)	Prec@1 94.922 (94.824)
Epoch: [62][136/176]	Time 0.072 (0.078)	Data 0.001 (0.005)	Loss 0.4915 (0.4198)	Prec@1 92.188 (94.776)
Epoch: [62][153/176]	Time 0.071 (0.077)	Data 0.001 (0.004)	Loss 0.3684 (0.4203)	Prec@1 94.922 (94.724)
Epoch: [62][170/176]	Time 0.070 (0.077)	Data 0.000 (0.004)	Loss 0.4364 (0.4206)	Prec@1 93.750 (94.673)
 * Prec@1 90.740
 * Prec@1 90.620
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 142, in main
    mean_prec1, std_prec1 = validate_noisy(val_loader, model, args, eta_inf=args.eta_train, eta_mode=args.eta_mode, n_inf=3)
  File "trainer_resnet.py", line 255, in validate_noisy
    accs[i] = validate(val_loader, model_noisy, args)
  File "trainer_resnet.py", line 278, in validate
    output = model(input_var)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 159, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Architectures/cifar_resnet.py", line 110, in forward
    out = self.layer3(out)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Architectures/cifar_resnet.py", line 76, in forward
    out = F.relu(self.bn1(self.conv1(x)))
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 423, in forward
    return self._conv_forward(input, self.weight)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 420, in _conv_forward
    self.padding, self.dilation, self.groups)
KeyboardInterrupt
