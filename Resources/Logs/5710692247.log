Sender: LSF System <lsfadmin@zhcc009>
Subject: Job 777166: <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.075 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5710692247> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.075 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5710692247> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:14 2021
Job was executed on host(s) <zhcc009>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:15 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:15 2021
Terminated at Tue Nov 23 12:50:29 2021
Results reported at Tue Nov 23 12:50:29 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.075 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=5710692247
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   240.27 sec.
    Max Memory :                                 2062 MB
    Average Memory :                             1394.38 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                131
    Run time :                                   79 sec.
    Turnaround time :                            75 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 99.440
 * Prec@1 14.440
 * Prec@1 14.380
 * Prec@1 13.420
 * Prec@1 15.180
 * Noisy Prec@1 14.33
After loading, with clipping@2.00 14.44 w/o 99.44 noisy: 14.33pm0.88
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.648 (0.648)	Data 0.552 (0.552)	Loss 0.9046 (0.9046)	Prec@1 88.672 (88.672)
Epoch: [60][17/176]	Time 0.094 (0.114)	Data 0.002 (0.032)	Loss 0.7143 (0.8127)	Prec@1 88.672 (89.887)
Epoch: [60][34/176]	Time 0.073 (0.099)	Data 0.002 (0.017)	Loss 0.7363 (0.7619)	Prec@1 87.109 (89.152)
Epoch: [60][51/176]	Time 0.071 (0.093)	Data 0.001 (0.012)	Loss 0.5830 (0.7115)	Prec@1 93.359 (89.949)
Epoch: [60][68/176]	Time 0.071 (0.089)	Data 0.001 (0.010)	Loss 0.4646 (0.6733)	Prec@1 95.312 (90.744)
Epoch: [60][85/176]	Time 0.081 (0.086)	Data 0.002 (0.008)	Loss 0.5606 (0.6408)	Prec@1 92.578 (91.365)
Epoch: [60][102/176]	Time 0.071 (0.084)	Data 0.001 (0.007)	Loss 0.4898 (0.6231)	Prec@1 94.922 (91.516)
Epoch: [60][119/176]	Time 0.073 (0.082)	Data 0.001 (0.006)	Loss 0.4553 (0.6051)	Prec@1 95.312 (91.836)
Epoch: [60][136/176]	Time 0.074 (0.081)	Data 0.001 (0.005)	Loss 0.4372 (0.5910)	Prec@1 94.531 (92.079)
Epoch: [60][153/176]	Time 0.091 (0.081)	Data 0.002 (0.005)	Loss 0.4548 (0.5766)	Prec@1 94.922 (92.307)
Epoch: [60][170/176]	Time 0.070 (0.081)	Data 0.000 (0.005)	Loss 0.5444 (0.5662)	Prec@1 91.406 (92.482)
 * Prec@1 84.540
 * Prec@1 77.680
 * Prec@1 80.940
 * Prec@1 79.020
 * Noisy Prec@1 79.21
	 * New best: 79.21333
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.585 (0.585)	Data 0.514 (0.514)	Loss 0.4872 (0.4872)	Prec@1 94.141 (94.141)
Epoch: [61][17/176]	Time 0.071 (0.099)	Data 0.001 (0.030)	Loss 0.4843 (0.4656)	Prec@1 94.141 (93.967)
Epoch: [61][34/176]	Time 0.071 (0.085)	Data 0.001 (0.016)	Loss 0.4620 (0.4665)	Prec@1 94.141 (93.650)
Epoch: [61][51/176]	Time 0.071 (0.081)	Data 0.001 (0.011)	Loss 0.4190 (0.4665)	Prec@1 96.484 (93.727)
Epoch: [61][68/176]	Time 0.071 (0.078)	Data 0.001 (0.009)	Loss 0.4210 (0.4551)	Prec@1 96.094 (94.027)
Epoch: [61][85/176]	Time 0.071 (0.077)	Data 0.001 (0.007)	Loss 0.4433 (0.4516)	Prec@1 94.922 (94.218)
Epoch: [61][102/176]	Time 0.071 (0.076)	Data 0.001 (0.006)	Loss 0.4929 (0.4528)	Prec@1 94.531 (94.220)
Epoch: [61][119/176]	Time 0.071 (0.076)	Data 0.001 (0.005)	Loss 0.4105 (0.4539)	Prec@1 94.531 (94.219)
Epoch: [61][136/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.5426 (0.4537)	Prec@1 93.359 (94.206)
Epoch: [61][153/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.3907 (0.4490)	Prec@1 94.141 (94.232)
Epoch: [61][170/176]	Time 0.069 (0.074)	Data 0.000 (0.004)	Loss 0.3770 (0.4484)	Prec@1 94.531 (94.198)
 * Prec@1 88.740
 * Prec@1 86.400
 * Prec@1 85.920
 * Prec@1 88.400
 * Noisy Prec@1 86.91
	 * New best: 86.90667
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.580 (0.580)	Data 0.489 (0.489)	Loss 0.5476 (0.5476)	Prec@1 92.969 (92.969)
Epoch: [62][17/176]	Time 0.073 (0.105)	Data 0.001 (0.029)	Loss 0.5011 (0.4161)	Prec@1 92.969 (94.010)
Epoch: [62][34/176]	Time 0.076 (0.090)	Data 0.002 (0.015)	Loss 0.4389 (0.4131)	Prec@1 94.531 (94.453)
Epoch: [62][51/176]	Time 0.071 (0.085)	Data 0.001 (0.011)	Loss 0.5426 (0.4202)	Prec@1 92.188 (94.328)
Epoch: [62][68/176]	Time 0.071 (0.081)	Data 0.001 (0.008)	Loss 0.3957 (0.4151)	Prec@1 94.531 (94.458)
Epoch: [62][85/176]	Time 0.071 (0.079)	Data 0.001 (0.007)	Loss 0.4027 (0.4212)	Prec@1 95.312 (94.409)
Epoch: [62][102/176]	Time 0.071 (0.078)	Data 0.001 (0.006)	Loss 0.4307 (0.4198)	Prec@1 92.188 (94.353)
Epoch: [62][119/176]	Time 0.071 (0.077)	Data 0.001 (0.005)	Loss 0.3917 (0.4169)	Prec@1 96.094 (94.408)
Epoch: [62][136/176]	Time 0.074 (0.077)	Data 0.002 (0.005)	Loss 0.4464 (0.4164)	Prec@1 94.922 (94.460)
Epoch: [62][153/176]	Time 0.073 (0.077)	Data 0.002 (0.004)	Loss 0.3335 (0.4194)	Prec@1 97.266 (94.409)
Epoch: [62][170/176]	Time 0.070 (0.077)	Data 0.000 (0.004)	Loss 0.3977 (0.4188)	Prec@1 95.312 (94.417)
 * Prec@1 89.240
 * Prec@1 87.880
 * Prec@1 88.260
 * Prec@1 87.760
 * Noisy Prec@1 87.97
	 * New best: 87.96667
current lr 2.00000e-02
Epoch: [63][0/176]	Time 0.600 (0.600)	Data 0.515 (0.515)	Loss 0.4734 (0.4734)	Prec@1 94.531 (94.531)
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 138, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 213, in train
    epoch=epoch - args.start_epoch
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Losses/torch_loss.py", line 153, in compute_gradient_and_backward
    nat_loss = self.natural_loss(model(X), y)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 159, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Architectures/cifar_resnet.py", line 109, in forward
    out = self.layer2(out)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/Resnet32-ICLR/Architectures/cifar_resnet.py", line 76, in forward
    out = F.relu(self.bn1(self.conv1(x)))
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 423, in forward
    return self._conv_forward(input, self.weight)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 420, in _conv_forward
    self.padding, self.dilation, self.groups)
KeyboardInterrupt
