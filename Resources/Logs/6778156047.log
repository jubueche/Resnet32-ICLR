Sender: LSF System <lsfadmin@zhcc009>
Subject: Job 777168: <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=6778156047> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=6778156047> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:15 2021
Job was executed on host(s) <zhcc009>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:16 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:16 2021
Terminated at Tue Nov 23 12:50:28 2021
Results reported at Tue Nov 23 12:50:28 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=6778156047
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   242.18 sec.
    Max Memory :                                 2006 MB
    Average Memory :                             1593.54 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                131
    Run time :                                   72 sec.
    Turnaround time :                            73 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 99.440
 * Prec@1 14.440
 * Prec@1 14.460
 * Prec@1 13.320
 * Prec@1 15.740
 * Noisy Prec@1 14.51
After loading, with clipping@2.00 14.44 w/o 99.44 noisy: 14.51pm1.21
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.670 (0.670)	Data 0.499 (0.499)	Loss 0.9046 (0.9046)	Prec@1 88.672 (88.672)
Epoch: [60][17/176]	Time 0.075 (0.115)	Data 0.001 (0.029)	Loss 0.7510 (0.8303)	Prec@1 86.719 (88.260)
Epoch: [60][34/176]	Time 0.071 (0.098)	Data 0.001 (0.016)	Loss 0.7696 (0.7802)	Prec@1 88.672 (87.991)
Epoch: [60][51/176]	Time 0.071 (0.091)	Data 0.001 (0.011)	Loss 0.5118 (0.7283)	Prec@1 94.141 (88.852)
Epoch: [60][68/176]	Time 0.071 (0.089)	Data 0.001 (0.009)	Loss 0.5164 (0.6833)	Prec@1 92.578 (89.714)
Epoch: [60][85/176]	Time 0.071 (0.086)	Data 0.001 (0.007)	Loss 0.5908 (0.6469)	Prec@1 93.359 (90.584)
Epoch: [60][102/176]	Time 0.071 (0.083)	Data 0.001 (0.006)	Loss 0.5098 (0.6274)	Prec@1 94.922 (91.038)
Epoch: [60][119/176]	Time 0.071 (0.082)	Data 0.001 (0.006)	Loss 0.4795 (0.6098)	Prec@1 94.922 (91.396)
Epoch: [60][136/176]	Time 0.071 (0.081)	Data 0.001 (0.005)	Loss 0.4832 (0.5935)	Prec@1 94.922 (91.694)
Epoch: [60][153/176]	Time 0.071 (0.081)	Data 0.001 (0.005)	Loss 0.3833 (0.5781)	Prec@1 96.484 (91.954)
Epoch: [60][170/176]	Time 0.082 (0.080)	Data 0.000 (0.004)	Loss 0.5686 (0.5680)	Prec@1 91.016 (92.178)
 * Prec@1 87.080
 * Prec@1 81.700
 * Prec@1 78.260
 * Prec@1 77.640
 * Noisy Prec@1 79.20
	 * New best: 79.20000
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.636 (0.636)	Data 0.559 (0.559)	Loss 0.4741 (0.4741)	Prec@1 93.359 (93.359)
Epoch: [61][17/176]	Time 0.071 (0.102)	Data 0.001 (0.032)	Loss 0.4680 (0.4442)	Prec@1 94.531 (94.227)
Epoch: [61][34/176]	Time 0.071 (0.087)	Data 0.001 (0.017)	Loss 0.3601 (0.4487)	Prec@1 96.875 (94.286)
Epoch: [61][51/176]	Time 0.072 (0.082)	Data 0.002 (0.012)	Loss 0.3891 (0.4498)	Prec@1 96.875 (94.238)
Epoch: [61][68/176]	Time 0.074 (0.080)	Data 0.001 (0.009)	Loss 0.4399 (0.4444)	Prec@1 95.703 (94.463)
Epoch: [61][85/176]	Time 0.072 (0.078)	Data 0.001 (0.008)	Loss 0.4611 (0.4437)	Prec@1 94.141 (94.490)
Epoch: [61][102/176]	Time 0.071 (0.077)	Data 0.001 (0.007)	Loss 0.4590 (0.4480)	Prec@1 93.359 (94.307)
Epoch: [61][119/176]	Time 0.071 (0.076)	Data 0.001 (0.006)	Loss 0.3684 (0.4505)	Prec@1 94.922 (94.209)
Epoch: [61][136/176]	Time 0.071 (0.076)	Data 0.001 (0.005)	Loss 0.5024 (0.4491)	Prec@1 91.016 (94.280)
Epoch: [61][153/176]	Time 0.071 (0.075)	Data 0.001 (0.005)	Loss 0.4389 (0.4470)	Prec@1 92.969 (94.288)
Epoch: [61][170/176]	Time 0.070 (0.075)	Data 0.000 (0.005)	Loss 0.3614 (0.4466)	Prec@1 97.266 (94.303)
 * Prec@1 87.120
 * Prec@1 78.000
 * Prec@1 83.120
 * Prec@1 83.880
 * Noisy Prec@1 81.67
	 * New best: 81.66666
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.588 (0.588)	Data 0.502 (0.502)	Loss 0.5264 (0.5264)	Prec@1 93.359 (93.359)
Epoch: [62][17/176]	Time 0.071 (0.107)	Data 0.001 (0.029)	Loss 0.5122 (0.4186)	Prec@1 92.969 (94.596)
Epoch: [62][34/176]	Time 0.071 (0.091)	Data 0.001 (0.016)	Loss 0.4797 (0.4297)	Prec@1 94.531 (94.487)
Epoch: [62][51/176]	Time 0.072 (0.085)	Data 0.003 (0.011)	Loss 0.5380 (0.4444)	Prec@1 91.406 (94.141)
Epoch: [62][68/176]	Time 0.071 (0.082)	Data 0.001 (0.009)	Loss 0.4519 (0.4379)	Prec@1 96.094 (94.209)
Epoch: [62][85/176]	Time 0.071 (0.080)	Data 0.001 (0.007)	Loss 0.4164 (0.4405)	Prec@1 94.922 (94.200)
Epoch: [62][102/176]	Time 0.085 (0.080)	Data 0.002 (0.006)	Loss 0.4310 (0.4353)	Prec@1 93.359 (94.273)
Epoch: [62][119/176]	Time 0.071 (0.078)	Data 0.001 (0.006)	Loss 0.3821 (0.4277)	Prec@1 94.922 (94.382)
Epoch: [62][136/176]	Time 0.072 (0.078)	Data 0.001 (0.005)	Loss 0.4403 (0.4253)	Prec@1 93.359 (94.457)
Epoch: [62][153/176]	Time 0.071 (0.078)	Data 0.001 (0.005)	Loss 0.3446 (0.4268)	Prec@1 96.484 (94.409)
Epoch: [62][170/176]	Time 0.070 (0.077)	Data 0.000 (0.004)	Loss 0.4737 (0.4269)	Prec@1 93.750 (94.403)
 * Prec@1 82.880
 * Prec@1 82.960
 * Prec@1 81.900
 * Prec@1 78.520
 * Noisy Prec@1 81.13
current lr 2.00000e-02
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 138, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 200, in train
    for i, (input, target) in enumerate(train_loader):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 435, in __next__
    data = self._next_data()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1068, in _next_data
    idx, data = self._get_data()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1024, in _get_data
    success, data = self._try_get_data()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 872, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/threading.py", line 300, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt
