Sender: LSF System <lsfadmin@zhcc003>
Subject: Job 768146: <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=3 -beta_robustness=0.1 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.3 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=4013374453> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=3 -beta_robustness=0.1 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.3 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=4013374453> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov  2 14:09:21 2021
Job was executed on host(s) <zhcc003>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov  2 14:09:22 2021
</u/jbu> was used as the home directory.
</u/jbu/Master-Thesis> was used as the working directory.
Started at Tue Nov  2 14:09:22 2021
Terminated at Tue Nov  2 14:13:14 2021
Results reported at Tue Nov  2 14:13:14 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=0 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=3 -beta_robustness=0.1 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.3 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=4013374453
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   366.92 sec.
    Max Memory :                                 2055 MB
    Average Memory :                             1856.17 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                131
    Run time :                                   232 sec.
    Turnaround time :                            233 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Master-Thesis/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 93.900
 * Prec@1 19.920
 * Prec@1 20.820
 * Prec@1 18.920
 * Prec@1 20.440
 * Noisy Prec@1 20.06
After loading, with clipping@2.00 19.92 w/o 93.90 noisy: 20.06pm1.01
current lr 2.00000e-02
Epoch: [60][0/176]	Time 1.067 (1.067)	Data 0.475 (0.475)	Loss 1.8644 (1.8644)	Prec@1 87.109 (87.109)
Epoch: [60][17/176]	Time 0.462 (0.501)	Data 0.001 (0.027)	Loss 1.6092 (1.7053)	Prec@1 90.234 (92.122)
Epoch: [60][34/176]	Time 0.461 (0.485)	Data 0.001 (0.015)	Loss 1.3923 (1.6221)	Prec@1 90.234 (91.083)
Epoch: [60][51/176]	Time 0.463 (0.478)	Data 0.001 (0.010)	Loss 1.2569 (1.5370)	Prec@1 91.016 (90.843)
Epoch: [60][68/176]	Time 0.470 (0.475)	Data 0.001 (0.008)	Loss 1.2161 (1.4579)	Prec@1 94.141 (91.016)
Epoch: [60][85/176]	Time 0.461 (0.474)	Data 0.001 (0.007)	Loss 1.0771 (1.4001)	Prec@1 91.797 (91.234)
Epoch: [60][102/176]	Time 0.463 (0.472)	Data 0.001 (0.006)	Loss 1.0328 (1.3513)	Prec@1 93.750 (91.425)
Epoch: [60][119/176]	Time 0.462 (0.472)	Data 0.001 (0.005)	Loss 1.0827 (1.3120)	Prec@1 90.234 (91.611)
Epoch: [60][136/176]	Time 0.487 (0.472)	Data 0.001 (0.005)	Loss 1.1347 (1.2762)	Prec@1 91.406 (91.671)
Epoch: [60][153/176]	Time 0.469 (0.471)	Data 0.001 (0.004)	Loss 0.9359 (1.2445)	Prec@1 93.359 (91.769)
Epoch: [60][170/176]	Time 0.563 (0.476)	Data 0.000 (0.004)	Loss 0.9733 (1.2165)	Prec@1 94.141 (91.925)
 * Prec@1 88.900
 * Prec@1 88.660
 * Prec@1 88.480
 * Prec@1 88.720
 * Noisy Prec@1 88.62
	 * New best: 88.62000
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.964 (0.964)	Data 0.492 (0.492)	Loss 1.0392 (1.0392)	Prec@1 92.188 (92.188)
Epoch: [61][17/176]	Time 0.469 (0.516)	Data 0.001 (0.028)	Loss 0.9177 (0.9297)	Prec@1 94.141 (93.359)
Epoch: [61][34/176]	Time 0.463 (0.496)	Data 0.001 (0.015)	Loss 0.8211 (0.9212)	Prec@1 94.922 (93.248)
Epoch: [61][51/176]	Time 0.461 (0.490)	Data 0.001 (0.011)	Loss 0.9031 (0.9129)	Prec@1 93.750 (93.434)
Epoch: [61][68/176]	Time 0.465 (0.487)	Data 0.001 (0.008)	Loss 0.8575 (0.9070)	Prec@1 93.359 (93.286)
Epoch: [61][85/176]	Time 0.462 (0.484)	Data 0.001 (0.007)	Loss 0.8659 (0.8980)	Prec@1 94.531 (93.491)
Epoch: [61][102/176]	Time 0.470 (0.482)	Data 0.001 (0.006)	Loss 0.8786 (0.8948)	Prec@1 92.188 (93.431)
Epoch: [61][119/176]	Time 0.463 (0.482)	Data 0.001 (0.005)	Loss 0.8025 (0.8858)	Prec@1 93.359 (93.464)
Epoch: [61][136/176]	Time 0.537 (0.483)	Data 0.002 (0.005)	Loss 0.8383 (0.8817)	Prec@1 93.359 (93.550)
Epoch: [61][153/176]	Time 0.462 (0.481)	Data 0.001 (0.004)	Loss 0.8347 (0.8800)	Prec@1 94.141 (93.481)
Epoch: [61][170/176]	Time 0.594 (0.480)	Data 0.000 (0.004)	Loss 0.8046 (0.8745)	Prec@1 91.797 (93.510)
 * Prec@1 89.640
 * Prec@1 89.320
 * Prec@1 89.460
 * Prec@1 89.100
 * Noisy Prec@1 89.29
	 * New best: 89.29333
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.949 (0.949)	Data 0.471 (0.471)	Loss 0.7749 (0.7749)	Prec@1 96.875 (96.875)
Epoch: [62][17/176]	Time 0.481 (0.496)	Data 0.001 (0.027)	Loss 0.7832 (0.8378)	Prec@1 96.484 (93.772)
Epoch: [62][34/176]	Time 0.462 (0.480)	Data 0.001 (0.015)	Loss 0.7646 (0.8285)	Prec@1 95.703 (93.594)
Epoch: [62][51/176]	Time 0.499 (0.476)	Data 0.002 (0.010)	Loss 0.8660 (0.8349)	Prec@1 92.188 (93.547)
Epoch: [62][68/176]	Time 0.463 (0.476)	Data 0.001 (0.008)	Loss 0.7378 (0.8283)	Prec@1 95.312 (93.648)
Epoch: [62][85/176]	Time 0.468 (0.475)	Data 0.001 (0.007)	Loss 0.8267 (0.8224)	Prec@1 92.578 (93.777)
Traceback (most recent call last):
  File "trainer_resnet.py", line 339, in <module>
    main()
  File "trainer_resnet.py", line 144, in main
    train(train_loader, model, criterion, optimizer, epoch, args=args, clip_fn=clip_weights)
  File "trainer_resnet.py", line 219, in train
    epoch=epoch - args.start_epoch
  File "/ibm/gpfs-homes/jbu/Master-Thesis/Losses/torch_loss.py", line 138, in compute_gradient_and_backward
    X
  File "/ibm/gpfs-homes/jbu/Master-Thesis/Losses/torch_loss.py", line 108, in _adversarial_loss
    loss_rob = self.L_rob(output_theta=self.model_theta(X), output_theta_star=output_theta_star)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 159, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/Master-Thesis/Architectures/cifar_resnet.py", line 108, in forward
    out = F.relu(self.bn1(self.conv1(x)))
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/ibm/gpfs-homes/jbu/AIS/ais/nn.py", line 608, in forward
    return self._conv_forward(input=input, weight=self.weight, padding=padding)
  File "/ibm/gpfs-homes/jbu/AIS/ais/nn.py", line 640, in _conv_forward
    weight = self._apply_noise(weight)
  File "/ibm/gpfs-homes/jbu/AIS/ais/nn.py", line 181, in _apply_noise
    noise = weight.abs().max() * self.config.ETA_TRAIN * torch.randn_like(weight) if self.config.ETA_MODE == "range" \
KeyboardInterrupt
