Sender: LSF System <lsfadmin@zhcc005>
Subject: Job 777178: <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=1870055394> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=1870055394> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:17 2021
Job was executed on host(s) <zhcc005>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:18 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:18 2021
Terminated at Tue Nov 23 12:50:28 2021
Results reported at Tue Nov 23 12:50:28 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.11 -eta_mode=ind -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=1870055394
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   221.65 sec.
    Max Memory :                                 2029 MB
    Average Memory :                             1393.08 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                133
    Run time :                                   70 sec.
    Turnaround time :                            71 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 99.440
 * Prec@1 14.440
 * Prec@1 14.580
 * Prec@1 16.480
 * Prec@1 13.620
 * Noisy Prec@1 14.89
After loading, with clipping@2.00 14.44 w/o 99.44 noisy: 14.89pm1.46
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.682 (0.682)	Data 0.587 (0.587)	Loss 0.9046 (0.9046)	Prec@1 88.672 (88.672)
Epoch: [60][17/176]	Time 0.072 (0.110)	Data 0.001 (0.034)	Loss 0.7774 (0.8168)	Prec@1 87.891 (90.039)
Epoch: [60][34/176]	Time 0.073 (0.094)	Data 0.002 (0.018)	Loss 0.7012 (0.7667)	Prec@1 87.500 (89.319)
Epoch: [60][51/176]	Time 0.075 (0.088)	Data 0.001 (0.013)	Loss 0.5110 (0.7146)	Prec@1 95.312 (90.174)
Epoch: [60][68/176]	Time 0.084 (0.086)	Data 0.002 (0.010)	Loss 0.5140 (0.6743)	Prec@1 91.406 (90.823)
Epoch: [60][85/176]	Time 0.072 (0.084)	Data 0.001 (0.008)	Loss 0.6147 (0.6409)	Prec@1 91.406 (91.384)
Epoch: [60][102/176]	Time 0.072 (0.082)	Data 0.001 (0.007)	Loss 0.4613 (0.6216)	Prec@1 94.531 (91.577)
Epoch: [60][119/176]	Time 0.072 (0.081)	Data 0.001 (0.006)	Loss 0.4485 (0.6011)	Prec@1 94.922 (91.940)
Epoch: [60][136/176]	Time 0.072 (0.080)	Data 0.001 (0.006)	Loss 0.5380 (0.5886)	Prec@1 93.750 (92.207)
Epoch: [60][153/176]	Time 0.081 (0.079)	Data 0.002 (0.005)	Loss 0.4782 (0.5765)	Prec@1 94.531 (92.375)
Epoch: [60][170/176]	Time 0.070 (0.078)	Data 0.000 (0.005)	Loss 0.5490 (0.5668)	Prec@1 92.578 (92.542)
 * Prec@1 87.000
 * Prec@1 84.840
 * Prec@1 82.880
 * Prec@1 81.880
 * Noisy Prec@1 83.20
	 * New best: 83.20000
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.560 (0.560)	Data 0.475 (0.475)	Loss 0.4842 (0.4842)	Prec@1 94.531 (94.531)
Epoch: [61][17/176]	Time 0.072 (0.103)	Data 0.001 (0.028)	Loss 0.4402 (0.4522)	Prec@1 92.969 (94.097)
Epoch: [61][34/176]	Time 0.078 (0.090)	Data 0.002 (0.015)	Loss 0.3480 (0.4444)	Prec@1 96.875 (94.386)
Epoch: [61][51/176]	Time 0.093 (0.084)	Data 0.002 (0.011)	Loss 0.4296 (0.4477)	Prec@1 95.312 (94.291)
Epoch: [61][68/176]	Time 0.072 (0.083)	Data 0.001 (0.008)	Loss 0.4501 (0.4415)	Prec@1 96.484 (94.458)
Epoch: [61][85/176]	Time 0.072 (0.082)	Data 0.001 (0.007)	Loss 0.3857 (0.4390)	Prec@1 95.703 (94.522)
Epoch: [61][102/176]	Time 0.072 (0.080)	Data 0.001 (0.006)	Loss 0.5070 (0.4418)	Prec@1 92.188 (94.467)
Epoch: [61][119/176]	Time 0.072 (0.079)	Data 0.001 (0.005)	Loss 0.4136 (0.4413)	Prec@1 94.922 (94.443)
Epoch: [61][136/176]	Time 0.072 (0.078)	Data 0.001 (0.005)	Loss 0.4749 (0.4398)	Prec@1 92.969 (94.503)
Epoch: [61][153/176]	Time 0.077 (0.077)	Data 0.001 (0.004)	Loss 0.4763 (0.4402)	Prec@1 94.922 (94.458)
Epoch: [61][170/176]	Time 0.070 (0.077)	Data 0.000 (0.004)	Loss 0.3880 (0.4408)	Prec@1 96.094 (94.476)
 * Prec@1 87.880
 * Prec@1 86.100
 * Prec@1 86.880
 * Prec@1 86.620
 * Noisy Prec@1 86.53
	 * New best: 86.53333
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.574 (0.574)	Data 0.488 (0.488)	Loss 0.5463 (0.5463)	Prec@1 94.141 (94.141)
Epoch: [62][17/176]	Time 0.072 (0.103)	Data 0.001 (0.028)	Loss 0.4992 (0.4216)	Prec@1 93.750 (95.204)
Epoch: [62][34/176]	Time 0.072 (0.088)	Data 0.001 (0.015)	Loss 0.4317 (0.4272)	Prec@1 94.922 (94.821)
Epoch: [62][51/176]	Time 0.072 (0.083)	Data 0.001 (0.011)	Loss 0.5092 (0.4393)	Prec@1 92.578 (94.464)
Epoch: [62][68/176]	Time 0.072 (0.080)	Data 0.002 (0.008)	Loss 0.4350 (0.4311)	Prec@1 94.141 (94.543)
Epoch: [62][85/176]	Time 0.073 (0.079)	Data 0.001 (0.007)	Loss 0.3979 (0.4319)	Prec@1 95.312 (94.563)
Epoch: [62][102/176]	Time 0.072 (0.078)	Data 0.002 (0.006)	Loss 0.4489 (0.4279)	Prec@1 94.531 (94.653)
Epoch: [62][119/176]	Time 0.072 (0.077)	Data 0.001 (0.005)	Loss 0.3739 (0.4241)	Prec@1 95.312 (94.639)
Epoch: [62][136/176]	Time 0.072 (0.076)	Data 0.001 (0.005)	Loss 0.4230 (0.4220)	Prec@1 93.750 (94.645)
Epoch: [62][153/176]	Time 0.072 (0.076)	Data 0.001 (0.004)	Loss 0.3826 (0.4241)	Prec@1 95.312 (94.572)
Epoch: [62][170/176]	Time 0.070 (0.075)	Data 0.000 (0.004)	Loss 0.4072 (0.4224)	Prec@1 96.484 (94.584)
Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 141, in main
    prec1 = validate(val_loader, model, args=args)
  File "trainer_resnet.py", line 284, in validate
    top1.update(prec1.item(), input.size(0))
KeyboardInterrupt
