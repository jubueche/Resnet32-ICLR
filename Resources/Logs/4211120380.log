Sender: LSF System <lsfadmin@zhcc004>
Subject: Job 777160: <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=4211120380> in cluster <ZHC2clusterLSF> Exited

Job <python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=4211120380> was submitted from host <zhcc022> by user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:12 2021
Job was executed on host(s) <zhcc004>, in queue <prod.med>, as user <jbu> in cluster <ZHC2clusterLSF> at Tue Nov 23 12:49:13 2021
</u/jbu> was used as the home directory.
</u/jbu/Resnet32-ICLR> was used as the working directory.
Started at Tue Nov 23 12:49:13 2021
Terminated at Tue Nov 23 12:50:29 2021
Results reported at Tue Nov 23 12:50:29 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 trainer_resnet.py -seed=1 -batch_size=256 -weight_decay=0.0005 -momentum=0.9 -save_every=10 -lr=0.1 -n_attack_steps=10 -beta_robustness=0.0 -gamma=0.0 -eps_pga=0.0 -eta_train=0.026 -eta_mode=range -clipping_alpha=2.0 -attack_size_mismatch=0.2 -initial_std=0.001 -pretrained -burn_in=0 -workers=4 -n_epochs=300 -dataset=cifar10 -architecture=resnet32 -start_epoch=60 -data_dir=/dataP/jbu -session_id=4211120380
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   241.37 sec.
    Max Memory :                                 2005 MB
    Average Memory :                             1329.23 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                133
    Run time :                                   77 sec.
    Turnaround time :                            77 sec.

The output (if any) follows:

Loaded pretrained model from /ibm/gpfs-homes/jbu/Resnet32-ICLR/Resources/cifar10_pretrained_models/resnet32.th
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 * Prec@1 99.440
 * Prec@1 14.440
 * Prec@1 14.140
 * Prec@1 13.940
 * Prec@1 14.500
 * Noisy Prec@1 14.19
After loading, with clipping@2.00 14.44 w/o 99.44 noisy: 14.19pm0.28
current lr 2.00000e-02
Epoch: [60][0/176]	Time 0.746 (0.746)	Data 0.645 (0.645)	Loss 0.9046 (0.9046)	Prec@1 88.672 (88.672)
Epoch: [60][17/176]	Time 0.084 (0.117)	Data 0.002 (0.037)	Loss 0.7041 (0.8129)	Prec@1 89.844 (89.518)
Epoch: [60][34/176]	Time 0.087 (0.100)	Data 0.002 (0.020)	Loss 0.6777 (0.7558)	Prec@1 91.016 (89.509)
Epoch: [60][51/176]	Time 0.076 (0.094)	Data 0.002 (0.014)	Loss 0.5401 (0.7057)	Prec@1 92.969 (90.054)
Epoch: [60][68/176]	Time 0.086 (0.090)	Data 0.002 (0.011)	Loss 0.5523 (0.6663)	Prec@1 92.578 (90.710)
Epoch: [60][85/176]	Time 0.083 (0.089)	Data 0.002 (0.009)	Loss 0.6337 (0.6343)	Prec@1 92.188 (91.384)
Epoch: [60][102/176]	Time 0.080 (0.087)	Data 0.002 (0.008)	Loss 0.4748 (0.6143)	Prec@1 93.359 (91.687)
Epoch: [60][119/176]	Time 0.074 (0.086)	Data 0.001 (0.007)	Loss 0.4799 (0.5970)	Prec@1 96.484 (92.051)
Epoch: [60][136/176]	Time 0.081 (0.086)	Data 0.001 (0.006)	Loss 0.5342 (0.5849)	Prec@1 92.188 (92.287)
Epoch: [60][153/176]	Time 0.071 (0.085)	Data 0.001 (0.006)	Loss 0.3969 (0.5726)	Prec@1 96.875 (92.472)
Epoch: [60][170/176]	Time 0.072 (0.084)	Data 0.000 (0.005)	Loss 0.5660 (0.5628)	Prec@1 90.234 (92.592)
 * Prec@1 86.880
 * Prec@1 87.140
 * Prec@1 86.740
 * Prec@1 86.420
 * Noisy Prec@1 86.77
	 * New best: 86.76666
current lr 2.00000e-02
Epoch: [61][0/176]	Time 0.719 (0.719)	Data 0.606 (0.606)	Loss 0.4860 (0.4860)	Prec@1 93.750 (93.750)
Epoch: [61][17/176]	Time 0.078 (0.123)	Data 0.001 (0.035)	Loss 0.4489 (0.4616)	Prec@1 94.922 (94.184)
Epoch: [61][34/176]	Time 0.085 (0.106)	Data 0.001 (0.019)	Loss 0.3462 (0.4558)	Prec@1 97.266 (94.208)
Epoch: [61][51/176]	Time 0.090 (0.098)	Data 0.002 (0.013)	Loss 0.4289 (0.4485)	Prec@1 96.484 (94.501)
Epoch: [61][68/176]	Time 0.072 (0.093)	Data 0.001 (0.010)	Loss 0.4890 (0.4474)	Prec@1 93.359 (94.627)
Epoch: [61][85/176]	Time 0.072 (0.090)	Data 0.001 (0.009)	Loss 0.4538 (0.4516)	Prec@1 94.141 (94.449)
Epoch: [61][102/176]	Time 0.072 (0.087)	Data 0.001 (0.007)	Loss 0.4639 (0.4514)	Prec@1 93.359 (94.380)
Epoch: [61][119/176]	Time 0.072 (0.085)	Data 0.001 (0.006)	Loss 0.4497 (0.4492)	Prec@1 94.922 (94.382)
Epoch: [61][136/176]	Time 0.071 (0.084)	Data 0.001 (0.006)	Loss 0.4879 (0.4468)	Prec@1 93.359 (94.460)
Epoch: [61][153/176]	Time 0.071 (0.082)	Data 0.001 (0.005)	Loss 0.4135 (0.4440)	Prec@1 93.359 (94.463)
Epoch: [61][170/176]	Time 0.069 (0.081)	Data 0.000 (0.005)	Loss 0.4094 (0.4429)	Prec@1 94.141 (94.431)
 * Prec@1 87.620
 * Prec@1 87.540
 * Prec@1 88.100
 * Prec@1 87.740
 * Noisy Prec@1 87.79
	 * New best: 87.79333
current lr 2.00000e-02
Epoch: [62][0/176]	Time 0.556 (0.556)	Data 0.463 (0.463)	Loss 0.5268 (0.5268)	Prec@1 92.969 (92.969)
Epoch: [62][17/176]	Time 0.079 (0.104)	Data 0.002 (0.027)	Loss 0.4586 (0.4228)	Prec@1 93.359 (94.488)
Epoch: [62][34/176]	Time 0.083 (0.093)	Data 0.002 (0.015)	Loss 0.4694 (0.4271)	Prec@1 94.141 (94.576)
Epoch: [62][51/176]	Time 0.071 (0.088)	Data 0.001 (0.010)	Loss 0.5370 (0.4300)	Prec@1 92.188 (94.449)
Epoch: [62][68/176]	Time 0.080 (0.086)	Data 0.002 (0.008)	Loss 0.4526 (0.4237)	Prec@1 96.094 (94.554)
Epoch: [62][85/176]	Time 0.072 (0.085)	Data 0.002 (0.007)	Loss 0.4110 (0.4264)	Prec@1 94.922 (94.577)
Epoch: [62][102/176]	Time 0.071 (0.082)	Data 0.001 (0.006)	Loss 0.4677 (0.4226)	Prec@1 91.406 (94.630)
Epoch: [62][119/176]	Time 0.074 (0.081)	Data 0.002 (0.005)	Loss 0.3597 (0.4199)	Prec@1 97.266 (94.697)
Epoch: [62][136/176]	Time 0.072 (0.080)	Data 0.001 (0.005)	Loss 0.4043 (0.4212)	Prec@1 94.922 (94.642)
Epoch: [62][153/176]	Time 0.072 (0.079)	Data 0.001 (0.004)	Loss 0.3395 (0.4234)	Prec@1 95.703 (94.562)
Epoch: [62][170/176]	Time 0.070 (0.079)	Data 0.000 (0.004)	Loss 0.4437 (0.4229)	Prec@1 94.922 (94.515)
 * Prec@1 89.200
Traceback (most recent call last):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/threading.py", line 300, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/threading.py", line 300, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "trainer_resnet.py", line 333, in <module>
    main()
  File "trainer_resnet.py", line 142, in main
    mean_prec1, std_prec1 = validate_noisy(val_loader, model, args, eta_inf=args.eta_train, eta_mode=args.eta_mode, n_inf=3)
  File "trainer_resnet.py", line 255, in validate_noisy
    accs[i] = validate(val_loader, model_noisy, args)
  File "trainer_resnet.py", line 272, in validate
    for i, (input, target) in enumerate(val_loader):
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 435, in __next__
    data = self._next_data()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1068, in _next_data
    idx, data = self._get_data()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1024, in _get_data
    success, data = self._try_get_data()
  File "/u/jbu/.conda/envs/msc/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 872, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/u/jbu/.conda/envs/msc/lib/python3.7/queue.py", line 182, in get
    return item
  File "/u/jbu/.conda/envs/msc/lib/python3.7/threading.py", line 244, in __exit__
    return self._lock.__exit__(*args)
RuntimeError: release unlocked lock
